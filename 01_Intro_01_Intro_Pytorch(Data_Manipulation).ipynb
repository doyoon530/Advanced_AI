{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Download and Install\n",
    "\n",
    "https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors represent (possibly multi-dimensional) arrays of numerical values.\n",
    "The simplest object we can create is a vector. To start, we can use `arange` to create a row vector with 12 consecutive integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float64)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the tensor shape through the shape attribute.\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .shape is an alias for .size(), and was added to more closely match numpy\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `reshape` function to change the shape of one (possibly multi-dimensional) array, to another that contains the same number of elements.\n",
    "\n",
    "For example, we can transform the shape of our line vector `x` to (3, 4), which contains the same values but interprets them as a matrix containing 3 rows and 4 columns.\n",
    "\n",
    "Note that although the shape has changed, the elements in `x` have not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reshape` 함수는 하나의 배열의 모양을 다른 모양으로 변경하는 데 사용됩니다. 이때 배열의 총 원소 수는 동일하게 유지됩니다.\n",
    "\n",
    "예를 들어, 주어진 코드에서는 선 벡터 `x`의 모양을 (3, 4)로 변환하여, 동일한 값들을 가지지만 이 값을 3개의 행과 4개의 열을 가진 행렬로 해석합니다.\n",
    "\n",
    "모양은 변경되었지만 `x`의 원소 값은 변경되지 않았음에 주의하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.],\n",
       "         [ 2.,  3.]],\n",
       "\n",
       "        [[ 4.,  5.],\n",
       "         [ 6.,  7.]],\n",
       "\n",
       "        [[ 8.,  9.],\n",
       "         [10., 11.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape((3, 2, 2))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, PyTorch can automatically work out one dimension given the other.\n",
    "\n",
    "We can invoke this capability by placing `-1` for the dimension that we would like PyTorch to automatically infer.\n",
    "\n",
    "In our case, instead of `x.reshape((3, 4))`, we could have equivalently used `x.reshape((-1, 4))` or `x.reshape((3, -1))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다행히 PyTorch는 다른 차원을 주어진 차원을 기반으로 자동으로 계산할 수 있습니다.\n",
    "\n",
    "이 기능을 사용하려면 PyTorch가 자동으로 추론하도록 차원에 대해 `-1`을 넣으면 됩니다.\n",
    "\n",
    "우리의 경우 `x.reshape((3, 4))` 대신에 `x.reshape((-1, 4))` 또는 `x.reshape((3, -1))`와 동등하게 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float64)\n",
    "x.reshape((-1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.],\n",
       "         [ 2.,  3.]],\n",
       "\n",
       "        [[ 4.,  5.],\n",
       "         [ 6.,  7.]],\n",
       "\n",
       "        [[ 8.,  9.],\n",
       "         [10., 11.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float64)\n",
    "x.reshape((3,2,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float64)\n",
    "x.reshape((3,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.FloatTensor(2, 3)\n",
    "print(x)\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\" > Pytorch uses float32 by default for tranining!!!! </span>\n",
    "\n",
    "torch.Tensor() is just an alias to torch.FloatTensor() which is the default type of tensor, when no dtype is specified during tensor construction.\n",
    "\n",
    "From the torch for numpy users notes, it seems that torch.Tensor() is a drop-in replacement of numpy.empty()\n",
    "\n",
    "So, in essence torch.FloatTensor() and torch.empty() does the same job.\n",
    "\n",
    "The `empty` method just grabs some memory and hands us back a matrix without setting the values of any of its entries. This is `very efficient but it means that the entries might take any arbitrary values`, including very big ones! Typically, we'll want our matrices initialized either with ones, zeros, some known constant or numbers randomly sampled from a known distribution.\n",
    "\n",
    "Perhaps most often, we want an array of all zeros. To create tensor with all elements set to 0 and a shape of (2, 3, 4) we can invoke:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 PyTorch는 훈련에 대해 float32를 사용합니다.\n",
    "\n",
    "`torch.Tensor()`은 단순히 `torch.FloatTensor()`에 대한 별칭으로, 텐서 생성 중 dtype이 지정되지 않은 경우의 기본 텐서 유형입니다.\n",
    "\n",
    "Torch for NumPy 사용자 노트에서 보면 `torch.Tensor()`은 `numpy.empty()`의 대체품인 것으로 나와있는데, 이것은 매우 효율적으로 동작하지만, 그 안에 있는 항목의 값을 설정하지 않으므로 임의의 값을 가질 수 있다는 것을 의미합니다. 일반적으로 우리는 행렬을 모두 1, 0, 일정한 상수 또는 알려진 분포에서 무작위로 샘플링된 숫자로 초기화하길 원할 것입니다.\n",
    "\n",
    "아마도 가장 자주 사용하는 것은 모든 원소가 0인 배열을 원하는 경우일 것입니다. 모든 원소가 0으로 설정되고 (2, 3, 4) 모양을 가진 텐서를 생성하려면 다음을 사용할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create tensors with each element set to 1 works via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify the value of each element in the desired NDArray by supplying a Python list containing the numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we will want to randomly sample the values of each element in the tensor according to some known probability distribution. This is especially common when we intend to use the tensor as a parameter in a neural network. The following snippet creates an tensor with a shape of (3,4). Each of its elements is randomly sampled in a normal distribution with zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연산\n",
    "\n",
    "자주 발생하는 상황 중 하나는 배열에 함수를 적용하려고 하는 경우입니다. 가장 간단하면서 가장 유용한 함수 중 일부는 원소별 함수입니다. 이러한 함수는 두 개의 배열의 해당 원소에 단일 스칼라 연산을 수행하여 작동합니다. 스칼라에서 스칼라로 매핑하는 함수로부터 원소별 함수를 만들 수 있습니다. 수학적 표기법에서는 이러한 함수를 $f: \\mathbb{R} \\rightarrow \\mathbb{R}$로 나타냅니다. 동일한 모양의 두 벡터 $\\mathbf{u}$ 및 $\\mathbf{v}$와 함수 $f$가 주어진 경우, 모든 $i$에 대해 $c_i \\gets f(u_i, v_i)$로 설정하여 벡터 $\\mathbf{c} = F(\\mathbf{u},\\mathbf{v})$를 생성할 수 있습니다. 여기서 스칼라 함수를 원소별 벡터 작업으로 *업그레이드*하여 벡터 값 함수 $F: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$를 생성했습니다. PyTorch에서는 공통 표준 산술 연산자 (+, -, /, *, **)가 임의 모양의 동일한 모양의 텐서에 대한 원소별 작업으로 *업그레이드*되었습니다. 행렬을 포함한 동일한 모양의 두 텐서에 대해 어떤 두 텐서에 대한 원소별 작업을 호출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([1., 2., 4., 8.])\n",
      "x + y tensor([ 3.,  4.,  6., 10.])\n",
      "x - y tensor([-1.,  0.,  2.,  6.])\n",
      "x * y tensor([ 2.,  4.,  8., 16.])\n",
      "x / y tensor([0.5000, 1.0000, 2.0000, 4.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 4, 8], dtype=torch.float32)\n",
    "y = torch.ones_like(x) * 2\n",
    "print('x =', x)\n",
    "print('x + y', x + y)\n",
    "print('x - y', x - y)\n",
    "print('x * y', x * y)\n",
    "print('x / y', x / y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many more operations can be applied element-wise, such as exponentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)\n",
    "# Note: torch.exp is not implemented for 'torch.LongTensor'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to computations by element, we can also perform matrix operations, like matrix multiplication using the `mm` or `matmul` function. Next, we will perform matrix multiplication of `x` and the transpose of `y`. We define `x` as a matrix of 3 rows and 4 columns, and `y` is transposed into a matrix of 4 rows and 3 columns. The two matrices are multiplied to obtain a matrix of 3 rows and 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([[2., 1., 4.],\n",
      "        [1., 2., 3.],\n",
      "        [4., 3., 2.],\n",
      "        [3., 4., 1.]])\n",
      "tensor([[ 18.,  20.,  10.],\n",
      "        [ 58.,  60.,  50.],\n",
      "        [ 98., 100.,  90.]])\n",
      "tensor([[ 18.,  20.,  10.],\n",
      "        [ 58.,  60.,  50.],\n",
      "        [ 98., 100.,  90.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 0에서 11까지의 수로 이루어진 1차원 텐서를 생성하고 이를 3x4 형태의 2차원 텐서로 변형합니다.\n",
    "x = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "\n",
    "# 3x4 형태의 텐서를 직접 지정하여 초기화합니다.\n",
    "# 데이터 유형은 float32로 설정합니다.\n",
    "y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]], dtype=torch.float32)\n",
    "\n",
    "# x의 데이터 유형을 출력합니다 (float32).\n",
    "print(x.dtype)\n",
    "\n",
    "# x 텐서의 내용을 출력합니다.\n",
    "print(x)\n",
    "\n",
    "# y 텐서를 전치하여 출력합니다 (행과 열을 바꿉니다).\n",
    "print(y.t())\n",
    "\n",
    "# x와 y의 행렬 곱을 계산하고 결과를 출력합니다.\n",
    "# 여기서 '*' 연산자는 행렬 곱이 아니라 원소별 곱입니다.\n",
    "# 따라서 .mm() 또는 .matmul() 함수를 사용하여 행렬 곱을 수행합니다.\n",
    "# 결과는 3x3 텐서입니다.\n",
    "print(torch.mm(x, y.t()))\n",
    "\n",
    "# 또 다른 방법으로 행렬 곱을 수행합니다.\n",
    "print(torch.matmul(x, y.t()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 4.],\n",
      "        [4., 3., 2., 1.],\n",
      "        [4., 3., 2., 1.]])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print((y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 4., 4.],\n",
      "        [2., 3., 3.],\n",
      "        [3., 2., 2.],\n",
      "        [4., 1., 1.]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "print(y.t())\n",
    "print((y.t()).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "print((x*y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[35.],\n",
      "        [23.],\n",
      "        [27.],\n",
      "        [27.],\n",
      "        [27.]])\n"
     ]
    }
   ],
   "source": [
    "s = torch.tensor([[60, 30, 10, 30], [10, 20, 30, 40], [40, 30, 20, 10], [40, 30, 20, 10],[40, 30, 20, 10]], dtype=torch.float32)\n",
    "weight = torch.tensor([[0.3, 0.3, 0.2, 0.2]], dtype=torch.float32)\n",
    "print(torch.mm(s, weight.t()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `torch.dot() behaves differently to np.dot()`. There's been some discussion about what would be desirable here. Specifically, `torch.dot() treats both a and b as 1D vectors` (irrespective of their original shape) and computes their `inner product`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.dot:\n",
      "[[ 18.  20.  10.]\n",
      " [ 58.  60.  50.]\n",
      " [ 98. 100.  90.]]\n",
      "torch.dot:168.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]], dtype=torch.float32)\n",
    "print(\"np.dot:\")\n",
    "print(np.dot(x,y.t()))\n",
    "\n",
    "x = torch.flatten(x)\n",
    "y = torch.flatten(y)\n",
    "x.shape, y.shape\n",
    "print(f\"torch.dot:{torch.dot(x,y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# numpy와 torch를 import합니다.\n",
    "\n",
    "x = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "# torch.arange를 사용하여 0에서 11까지의 값을 생성하고 dtype을 float32로 설정한 후 (3,4) 모양의 텐서로 변환합니다.\n",
    "# 결과적으로 x는 다음과 같이 생겼습니다:\n",
    "# tensor([[ 0.,  1.,  2.,  3.],\n",
    "#         [ 4.,  5.,  6.,  7.],\n",
    "#         [ 8.,  9., 10., 11.]])\n",
    "\n",
    "y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]], dtype=torch.float32)\n",
    "# 주어진 리스트를 사용하여 torch.tensor를 생성하고 dtype을 float32로 설정합니다.\n",
    "# 결과적으로 y는 다음과 같이 생겼습니다:\n",
    "# tensor([[2., 1., 4., 3.],\n",
    "#         [1., 2., 3., 4.],\n",
    "#         [4., 3., 2., 1.]])\n",
    "\n",
    "print(\"np.dot:\")\n",
    "# \"np.dot:\" 문자열을 출력합니다.\n",
    "\n",
    "# 아래에서는 numpy의 dot 함수를 사용하여 두 텐서의 내적을 계산하고 출력합니다.\n",
    "print(np.dot(x, y.t()))\n",
    "# x와 y의 전치(transpose)를 내적합니다. numpy의 dot 함수는 내적을 수행하는 함수입니다.\n",
    "# 결과는 다음과 같이 계산됩니다:\n",
    "# array([[ 18.,  20.,  10.],\n",
    "#        [ 58.,  60.,  50.],\n",
    "#        [ 98., 100.,  90.]])\n",
    "# 결과는 3x3 크기의 numpy 배열입니다.\n",
    "\n",
    "x = torch.flatten(x)\n",
    "# x를 1차원 텐서로 평평하게(flatten) 합니다.\n",
    "# 결과적으로 x는 다음과 같이 생겼습니다: tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n",
    "y = torch.flatten(y)\n",
    "# y도 마찬가지로 1차원 텐서로 평평하게(flatten) 합니다.\n",
    "# 결과적으로 y는 다음과 같이 생겼습니다: tensor([2., 1., 4., 3., 1., 2., 3., 4., 4., 3., 2., 1.])\n",
    "\n",
    "x.shape, y.shape\n",
    "# x와 y의 모양(shape)을 출력합니다.\n",
    "\n",
    "print(f\"torch.dot: {torch.dot(x, y)}\")\n",
    "# torch.dot 함수를 사용하여 두 1차원 텐서 x와 y의 내적을 계산하고 출력합니다.\n",
    "# 결과는 내적 값이 출력됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 두 1x3 형태의 텐서를 생성합니다. 데이터 유형은 float32로 설정합니다.\n",
    "x = torch.tensor([[1, 2, 3]], dtype=torch.float32)\n",
    "y = torch.tensor([[0, 1, 2]], dtype=torch.float32)\n",
    "\n",
    "# NumPy의 np.dot() 함수를 사용하여 두 텐서의 내적을 계산하고 결과를 출력합니다.\n",
    "# 내적은 두 벡터의 요소별 곱셈의 합입니다.\n",
    "# 결과는 스칼라 값입니다.\n",
    "print(np.dot(x, y.t()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also merge multiple tensors. For that, we need to tell the system along which dimension to merge. The example below merges two matrices along dimension 0 (along rows) and dimension 1 (along columns) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.],\n",
      "        [ 4.,  3.,  2.,  1.]])\n",
      "torch.Size([6, 4])\n",
      "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
      "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n",
      "torch.Size([3, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]], dtype=torch.float32)\n",
    "z=torch.cat((x, y), dim=0)\n",
    "print(z)\n",
    "print(z.shape)\n",
    "z=torch.cat((x, y), dim=1)\n",
    "print(z)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "# 0에서 11까지의 값을 가지는 3x4 모양의 torch 텐서 x를 생성합니다.\n",
    "# 결과적으로 x는 다음과 같이 생겼습니다:\n",
    "# tensor([[ 0.,  1.,  2.,  3.],\n",
    "#         [ 4.,  5.,  6.,  7.],\n",
    "#         [ 8.,  9., 10., 11.]])\n",
    "\n",
    "y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]], dtype=torch.float32)\n",
    "# 주어진 리스트를 사용하여 torch 텐서 y를 생성합니다.\n",
    "# 결과적으로 y는 다음과 같이 생겼습니다:\n",
    "# tensor([[2., 1., 4., 3.],\n",
    "#         [1., 2., 3., 4.],\n",
    "#         [4., 3., 2., 1.])\n",
    "\n",
    "z = torch.cat((x, y), dim=0)\n",
    "# torch.cat 함수를 사용하여 x와 y를 수직 방향(dim=0)으로 연결합니다.\n",
    "# 결과적으로 z는 x와 y를 수직으로 연결한 6x4 모양의 텐서입니다.\n",
    "# 행의 수가 증가하므로 x와 y의 열 수는 동일해야 합니다.\n",
    "# 결과는 다음과 같이 생겼습니다:\n",
    "# tensor([[ 0.,  1.,  2.,  3.],\n",
    "#         [ 4.,  5.,  6.,  7.],\n",
    "#         [ 8.,  9., 10., 11.],\n",
    "#         [ 2.,  1.,  4.,  3.],\n",
    "#         [ 1.,  2.,  3.,  4.],\n",
    "#         [ 4.,  3.,  2.,  1.]])\n",
    "\n",
    "print(z)\n",
    "# z를 출력합니다.\n",
    "print(z.shape)\n",
    "# z의 모양(shape)을 출력합니다. 결과는 (6, 4)입니다.\n",
    "\n",
    "z = torch.cat((x, y), dim=1)\n",
    "# torch.cat 함수를 사용하여 x와 y를 수평 방향(dim=1)으로 연결합니다.\n",
    "# 결과적으로 z는 x와 y를 수평으로 연결한 3x8 모양의 텐서입니다.\n",
    "# 열의 수가 증가하므로 x와 y의 행 수는 동일해야 합니다.\n",
    "# 결과는 다음과 같이 생겼습니다:\n",
    "# tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
    "#         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
    "#         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n",
    "\n",
    "print(z)\n",
    "# z를 출력합니다.\n",
    "print(z.shape)\n",
    "# z의 모양(shape)을 출력합니다. 결과는 (3, 8)입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "torch.Size([3, 4]) \n",
      "\n",
      "tensor([[2., 1., 4., 3.],\n",
      "        [1., 2., 3., 4.]])\n",
      "torch.Size([2, 4]) \n",
      "\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.]])\n",
      "torch.Size([5, 4]) \n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 3 but got size 2 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/ggam/Desktop/gnu/Advanced_AI/01_Intro_01_Intro_Pytorch(Data_Manipulation).ipynb 셀 42\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ggam/Desktop/gnu/Advanced_AI/01_Intro_01_Intro_Pytorch%28Data_Manipulation%29.ipynb#Y134sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(z)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ggam/Desktop/gnu/Advanced_AI/01_Intro_01_Intro_Pytorch%28Data_Manipulation%29.ipynb#Y134sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(z\u001b[39m.\u001b[39mshape,\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ggam/Desktop/gnu/Advanced_AI/01_Intro_01_Intro_Pytorch%28Data_Manipulation%29.ipynb#Y134sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m z\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39;49mcat((x, y), dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 3 but got size 2 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4]], dtype=torch.float32)\n",
    "print(x)\n",
    "print(x.shape,\"\\n\")\n",
    "\n",
    "print(y)\n",
    "print(y.shape,\"\\n\")\n",
    "\n",
    "z=torch.cat((x, y), dim=0)\n",
    "print(z)\n",
    "print(z.shape,\"\\n\")\n",
    "\n",
    "z=torch.cat((x, y), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we may want to construct binary tensors via logical statements. Take `x == y` as an example. If `x` and `y` are equal for some entry, the new tensor has a value of 1 at the same position; otherwise it is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x == y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True, False,  True],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "print(x==y)\n",
    "z=y\n",
    "print(z==y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing all the elements in the tensor yields an tensor with only one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([130., 100., 100., 100., 100.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([190., 140., 100., 100.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "x = x.reshape(3,4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can transform the result into a scalar in Python using the `a.item()`. In the following example, the $\\ell_2$ norm of `x` yields a single element tensor. `The final result is transformed into a scalar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.494443893432617\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print( x.norm().item() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Mechanism\n",
    "\n",
    "In the above section, we saw how to perform operations on two tensors of the same shape. When their shapes differ, a broadcasting mechanism may be triggered analogous to NumPy: first, copy the elements appropriately so that the two tensors have the same shape, and then carry out operations by element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.],\n",
       "         [1.],\n",
       "         [2.]]),\n",
       " tensor([[0., 1.]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3, dtype=torch.float).reshape((3, 1))\n",
    "b = torch.arange(2, dtype=torch.float).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `a` and `b` are (3x1) and (1x2) matrices respectively, their shapes do not match up if we want to add them. PyTorch addresses this by 'broadcasting' the entries of both matrices into a larger (3x2) matrix as follows: for matrix `a` it replicates the columns, for matrix `b` it replicates the rows before adding up both element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [1., 2.],\n",
       "        [2., 3.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing and Slicing\n",
    "\n",
    "Just like in any other Python array, elements in a tensor can be accessed by its index. In good Python tradition the first element has index 0 and ranges are specified to include the first but not the last element. By this logic `1:3` selects the second and third element. Let's try this out by selecting the respective rows in a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond reading, we can also write elements of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1, 2] = 9\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to assign multiple elements the same value, we simply index all of them and then assign them the value. For instance, `[0:2, :]` accesses the first and second rows. While we discussed indexing for matrices, this obviously also works for vectors and for tensors of more than 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0:2, :] = 12\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Memory\n",
    "\n",
    "In the previous example, every time we ran an operation, we allocated new memory to host its results.\n",
    "\n",
    "For example, if we write `y = x + y`, we will dereference the matrix that `y` used to point to and instead point it at the newly allocated memory.\n",
    "\n",
    "In the following example we demonstrate this with Python's `id()` function, which gives us the exact address of the referenced object in memory.\n",
    "\n",
    "After running `y = y + x`, we will find that `id(y)` points to a different location.\n",
    "\n",
    "That is because Python first evaluates `y + x`, allocating new memory for the result and then subsequently redirects `y` to point at this new location in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(y)\n",
    "y = y + x\n",
    "id(y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might be undesirable for two reasons. First, we do not want to run around allocating memory unnecessarily all the time. In machine learning, we might have hundreds of megabytes of parameters and update all of them multiple times per second. Typically, we will want to perform these updates *in place*. Second, we might point at the same parameters from multiple variables. If we do not update in place, this could cause a memory leak, making it possible for us to inadvertently reference stale parameters.\n",
    "\n",
    "Fortunately, performing in-place operations in PyTorch is easy. We can assign the result of an operation to a previously allocated array with slice notation, e.g., `y[:] = <expression>`. To illustrate the behavior, we first clone the shape of a matrix using `zeros_like` to allocate a block of 0 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것은 두 가지 이유로 바람직하지 않을 수 있습니다. 첫째, 우리는 메모리를 불필요하게 할당하고 다니길 원하지 않습니다. 기계 학습에서는 수백 메가바이트의 매개 변수를 가지고 있을 수 있으며 이러한 매개 변수를 초당 여러 번 업데이트하고 싶을 것입니다. 일반적으로 우리는 이러한 업데이트를 *in place*에서 수행하려고 합니다. 둘째, 동일한 매개 변수를 여러 변수에서 가리킬 수 있습니다. 우리가 *in place*로 업데이트하지 않으면 이것은 메모리 누수를 발생시킬 수 있으며 우리가 잘못된 매개 변수를 참조하게 만들 수 있습니다.\n",
    "\n",
    "다행스럽게도, PyTorch에서 *in-place* 연산을 수행하는 것은 쉽습니다. 우리는 슬라이스 표기법을 사용하여 이미 할당된 배열에 연산의 결과를 할당할 수 있습니다. 예를 들어, `y[:] = <expression>`와 같이 표현할 수 있습니다. 이 동작을 설명하기 위해 먼저 `zeros_like`를 사용하여 행렬의 모양을 복제하여 0 항목의 블록을 할당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros_like(y)\n",
    "print('id(z):', id(z))\n",
    "z[:] = x + y\n",
    "print('id(z):', id(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this looks pretty, `x+y` here will still allocate a temporary buffer to store the result of `x+y` before copying it to `z[:]`. To make even better use of memory, we can directly invoke the underlying `tensor` operation, in this case `add`, avoiding temporary buffers. We do this by specifying the `out` keyword argument, which every `tensor` operator supports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = id(z)\n",
    "torch.add(x, y, out=z)\n",
    "id(z) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the value of `x ` is not reused in subsequent computations, we can also use `x[:] = x + y` or `x += y` to reduce the memory overhead of the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = id(x)\n",
    "x += y\n",
    "id(x) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Mutual Transformation of PyTorch and NumPy`\n",
    "\n",
    "Converting PyTorch Tensors to and from NumPy Arrays is easy.\n",
    "\n",
    "The converted arrays do *not* share memory.\n",
    "\n",
    "This minor inconvenience is actually quite important: when you perform operations on the CPU or one of the GPUs, you do not want PyTorch having to wait whether NumPy might want to be doing something else with the same chunk of memory. `.tensor` and `.numpy` do the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "a = x.numpy()\n",
    "print(type(a))\n",
    "b = torch.tensor(a)\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:yellow\"> Pytorch Tutorial </span>\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9970af6c6272f912aea970d3aa30162bd36dbb3ebdad53a40420a930d21aa1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
