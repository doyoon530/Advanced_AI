{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalars\n",
    "\n",
    "If you never studied linear algebra or machine learning,\n",
    "you are probably used to working with one number at a time.\n",
    "And know how to do basic things like add them together or multiply them.\n",
    "\n",
    "For example, in Palo Alto, the temperature is $52$ degrees Fahrenheit.\n",
    "Formally, we call these values $scalars$.\n",
    "\n",
    "If you wanted to convert this value to Celsius (using metric system's more sensible unit of temperature measurement),\n",
    "you would evaluate the expression\n",
    "\n",
    "$c = (f - 32) * 5/9$\n",
    "\n",
    "setting $f$ to $52$.\n",
    "\n",
    "\n",
    "In this equation, each of the terms $32$, $5$, and $9$ is a scalar value.\n",
    "The placeholders $c$ and $f$ that we use are called variables\n",
    "and they represent unknown scalar values.\n",
    "\n",
    "In mathematical notation, we represent scalars with ordinary lower-cased letters ($x$, $y$, $z$).\n",
    "We also denote the space of all scalars as $\\mathcal{R}$.\n",
    "For expedience, we are going to punt a bit on what precisely a space is,\n",
    "but for now, remember that if you want to say that $x$ is a scalar,\n",
    "you can simply say $x \\in \\mathcal{R}$.\n",
    "The symbol $\\in$ can be pronounced \"in\" and just denotes membership in a set.\n",
    "\n",
    "In PyTorch, we work with scalars by creating tensors with just one element.\n",
    "In this snippet, we instantiate two scalars and perform some familiar arithmetic operations with them, such as addition, multiplication, division and exponentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스칼라 (Scalars)\n",
    "\n",
    "만약 선형 대수나 기계 학습을 공부한 적이 없다면, 아마도 한 번에 하나의 숫자를 다루는 데 익숙할 것입니다. 이러한 값들을 공식적으로 \"스칼라\"라고 합니다.\n",
    "\n",
    "예를 들어, Palo Alto의 온도가 화씨 52도이면, 이 값을 섭씨로 변환하려면 다음 식을 계산할 것입니다.\n",
    "\n",
    "$c = (f - 32) \\times \\frac{5}{9}$\n",
    "\n",
    "여기서 $f$ 값을 52로 설정합니다.\n",
    "\n",
    "이 식에서 $32$, $5$, $9$ 각각의 항은 스칼라 값입니다. 우리가 사용하는 $c$와 $f$ 같은 자리 표시자들은 \"변수\"라고 하며 알려지지 않은 스칼라 값들을 나타냅니다.\n",
    "\n",
    "수학적 표기법에서는 스칼라를 보통 일반 소문자로 ($x$, $y$, $z$) 나타냅니다. 또한 모든 스칼라의 공간을 $\\mathcal{R}$로 나타냅니다. 여기서 \"공간\"이 정확히 무엇인지에 대해서는 조금 생략하겠지만, 일단은 $x$가 스칼라임을 나타내려면 간단히 $x \\in \\mathcal{R}$라고 말할 수 있다는 것을 기억해 주세요. $\\in$ 기호는 \"속한다\"로 발음하며 집합의 멤버십을 나타냅니다.\n",
    "\n",
    "PyTorch에서는 하나의 요소만을 가지는 텐서를 생성하여 스칼라와 작업합니다. 이 코드 스니펫에서는 두 개의 스칼라를 인스턴스화하고, 그들 사이에 덧셈, 곱셈, 나눗셈 및 거듭제곱과 같은 익숙한 산술 연산을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y =  tensor([5.])\n",
      "x * y =  tensor([6.])\n",
      "x / y =  tensor([1.5000])\n",
      "x ** y =  tensor([9.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.0])\n",
    "y = torch.tensor([2.0])\n",
    "\n",
    "print('x + y = ', x + y)\n",
    "print('x * y = ', x * y)\n",
    "print('x / y = ', x / y)\n",
    "print('x ** y = ', torch.pow(x,y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert any tensor to a Python float by calling its `item` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors\n",
    "\n",
    "You can think of a vector as simply a list of numbers, for example ``[1.0,3.0,4.0,2.0]``.\n",
    "Each of the numbers in the vector consists of a single scalar value.\n",
    "We call these values the *entries* or *components* of the vector.\n",
    "Often, we are interested in vectors whose values hold some real-world significance.\n",
    "For example, if we are studying the risk that loans default,\n",
    "we might associate each applicant with a vector\n",
    "whose components correspond to their income,\n",
    "length of employment, number of previous defaults, etc.\n",
    "If we were studying the risk of heart attacks hospital patients potentially face,\n",
    "we might represent each patient with a vector\n",
    "whose components capture their most recent vital signs,\n",
    "cholesterol levels, minutes of exercise per day, etc.\n",
    "In math notation, we will usually denote vectors as bold-faced,\n",
    "lower-cased letters ($\\mathbf{u}$, $\\mathbf{v}$, $\\mathbf{w})$.\n",
    "In PyTorch, we work with vectors via 1D tensors with an arbitrary number of components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벡터 (Vectors)\n",
    "\n",
    "벡터는 단순히 숫자 목록으로 생각할 수 있습니다. 예를 들어, ``[1.0, 3.0, 4.0, 2.0]``와 같은 것입니다. 벡터 내의 각 숫자는 하나의 스칼라 값으로 구성됩니다. 이러한 값들을 벡터의 *요소* 또는 *구성 요소*라고 부릅니다. 종종, 우리는 값들이 현실 세계의 의미를 가지는 벡터에 관심이 있습니다. 예를 들어, 대출의 기본 위험을 연구하는 경우, 각 신청자를 수입, 고용 기간, 이전 기본 수 등에 해당하는 요소를 가진 벡터와 관련시킬 수 있습니다. 만약 환자들이 잠재적으로 직면하는 심장 발작 위험을 연구한다면, 각 환자를 최근 생체 신호, 콜레스테롤 수치, 일일 운동 시간 등을 포함하는 벡터로 나타낼 수 있습니다. 수학 표기법에서는 보통 벡터를 굵은 소문자로 ($\\mathbf{u}$, $\\mathbf{v}$, $\\mathbf{w}$) 나타냅니다. PyTorch에서는 임의의 구성 요소를 가진 1차원 텐서를 사용하여 벡터와 작업합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  tensor([0, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "print('x = ', x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refer to any element of a vector by using a subscript.\n",
    "For example, we can refer to the $4$th element of $\\mathbf{u}$ by $u_4$.\n",
    "Note that the element $u_4$ is a scalar,\n",
    "so we do not bold-face the font when referring to it.\n",
    "In code, we access any element $i$ by indexing into the ``tensor``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length, dimensionality and shape\n",
    "\n",
    "Let's revisit some concepts from the previous section. A vector is just an array of numbers.\n",
    "\n",
    "And just as every array has a length, so does every vector.\n",
    "\n",
    "In math notation, if we want to say that a vector $\\mathbf{x}$ consists of $n$ real-valued scalars,\n",
    "we can express this as $\\mathbf{x} \\in \\mathcal{R}^n$.\n",
    "\n",
    "The length of a vector is commonly called its $dimension$.\n",
    "\n",
    "As with an ordinary Python array, we can access the length of a tensor\n",
    "by calling Python's in-built ``len()`` function.\n",
    "\n",
    "We can also access a vector's length via its `.shape` attribute.\n",
    "The shape is a tuple that lists the dimensionality of the tensor along each of its axes.\n",
    "\n",
    "Because a vector can only be indexed along one axis, its shape has just one element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 길이, 차원 및 형태\n",
    "\n",
    "이전 섹션에서 다룬 몇 가지 개념을 다시 살펴보겠습니다. 벡터는 단순히 숫자의 배열입니다.\n",
    "\n",
    "그리고 모든 배열이 길이를 가지듯이 모든 벡터도 길이를 가집니다.\n",
    "\n",
    "수학적 표기법에서 벡터 $\\mathbf{x}$가 $n$개의 실수 스칼라로 구성된다고 말하려면 이를 $\\mathbf{x} \\in \\mathcal{R}^n$으로 표현할 수 있습니다.\n",
    "\n",
    "벡터의 길이는 일반적으로 그 차원(dimension)이라고 부릅니다.\n",
    "\n",
    "일반적인 Python 배열처럼 텐서의 길이에 접근하기 위해 Python의 내장 `len()` 함수를 호출할 수 있습니다.\n",
    "\n",
    "또한 벡터의 길이는 `.shape` 속성을 통해 액세스할 수 있습니다. 형태(shape)는 각 축을 따라 텐서의 차원을 나열한 튜플입니다.\n",
    "\n",
    "벡터는 하나의 축을 따라만 색인화할 수 있기 때문에 그 형태에는 하나의 요소만 포함됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 1, 2, 3]]]) torch.Size([1, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "x = x.reshape((1,1,4))\n",
    "print(x,x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the word dimension is overloaded and this tends to confuse people.<br>\n",
    "Some use the *dimensionality* of a vector to refer to its length (the number of components).<br>\n",
    "However some use the word *dimensionality* to refer to the number of axes that an array has.<br>\n",
    "In this sense, a scalar *would have* $0$ dimensions and a vector *would have* $1$ dimension.<br>\n",
    "**To avoid confusion, when we say *2D* array or *3D* array, we mean an array with 2 or 3 axes respectively. But if we say *$n$-dimensional* vector, we mean a vector of length $n$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "차원(dimension)이라는 단어는 혼란스럽게 사용되는 경향이 있으며, 이로 인해 사람들을 혼랍게 만들 수 있습니다.\n",
    "\n",
    "일부 사람들은 벡터의 *차원성*을 그 길이(구성 요소의 수)를 나타내는 데 사용합니다. 그러나 일부 사람들은 *차원성*이라는 단어를 배열이 가지는 축의 수를 나타내는 데 사용합니다. 이 의미에서 스칼라는 *0* 차원을 가지고 벡터는 *1* 차원을 가질 것입니다.\n",
    "\n",
    "**혼동을 피하기 위해, 우리가 *2D* 배열 또는 *3D* 배열이라고 말할 때, 우리는 각각 2개 또는 3개의 축을 가진 배열을 의미합니다. 그러나 *$n$-차원* 벡터라고 말할 때, 우리는 길이가 $n$인 벡터를 의미합니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4, 6])\n",
      "tensor([12, 24, 36])\n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "x = torch.tensor([1,2,3])\n",
    "y = torch.tensor([10,20,30])\n",
    "print(a * x)\n",
    "print(a * x + y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "\n",
    "Just as vectors generalize scalars from order $0$ to order $1$,\n",
    "matrices generalize vectors from $1D$ to $2D$.\n",
    "Matrices, which we'll typically denote with capital letters ($A$, $B$, $C$),\n",
    "are represented in code as tensors with 2 axes.\n",
    "Visually, we can draw a matrix as a table,\n",
    "where each entry $a_{ij}$ belongs to the $i$-th row and $j$-th column.\n",
    "\n",
    "\n",
    "$$A=\\begin{pmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nm} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "We can create a matrix with $n$ rows and $m$ columns in PyTorch\n",
    "by specifying a shape with two components `(n,m)`\n",
    "when calling any of our favorite functions for instantiating an `tensor`\n",
    "such as `ones`, or `zeros`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매트릭스\n",
    "\n",
    "스칼라를 0차원에서 1차원으로 일반화하는 것처럼,\n",
    "매트릭스는 1D에서 2D로 벡터를 일반화합니다.\n",
    "매트릭스는 보통 대문자로 나타내며 ($A$, $B$, $C$) 코드에서는 2개의 축을 가진 텐서로 표현됩니다.\n",
    "시각적으로, 매트릭스를 테이블로 그릴 수 있으며, 각 항목 $a_{ij}$은 $i$-번째 행과 $j$-번째 열에 속합니다.\n",
    "\n",
    "$$A=\\begin{pmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nm} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "우리는 PyTorch에서 $n$개의 행과 $m$개의 열을 가진 매트릭스를 만들 수 있으며, 이를 위해 `ones` 또는 `zeros`와 같은 텐서를 인스턴스화하기 위해 호출할 때 형태에 두 가지 구성 요소 `(n,m)`을 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape((5,4))\n",
    "print(A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices are useful data structures: they allow us to organize data that has different modalities of variation.<br>\n",
    "For example, rows in our matrix might correspond to different patients, while columns might correspond to different attributes.<br>\n",
    "We can access the scalar elements $a_{ij}$ of a matrix $A$ by specifying the indices for the row ($i$) and column ($j$) respectively.<br>\n",
    "Leaving them blank via a `:` takes all elements along the respective dimension (as seen in the previous section).<br>\n",
    "We can transpose the matrix through `t()`. That is, if $B = A^T$, then $b_{ij} = a_{ji}$ for any $i$ and $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매트릭스는 유용한 데이터 구조입니다. 이들을 사용하면 변동의 다양한 모드를 조직화할 수 있습니다.\n",
    "\n",
    "예를 들어, 매트릭스의 행은 서로 다른 환자에 해당할 수 있고, 열은 서로 다른 속성에 해당할 수 있습니다.\n",
    "\n",
    "매트릭스 $A$의 스칼라 요소 $a_{ij}$에 접근하려면 행 ($i$) 및 열 ($j$)에 대한 색인을 지정합니다. `:`을 사용하여 해당 차원에 따라 모든 요소를 가져올 수 있습니다 (이전 섹션에서 볼 수 있듯이).\n",
    "\n",
    "매트릭스를 전치(transpose)하려면 `t()`를 사용합니다. 즉, $B = A^T$이면 $b_{ij} = a_{ji}$가 모든 $i$와 $j$에 대해 성립합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  4.,  8., 12., 16.],\n",
      "        [ 1.,  5.,  9., 13., 17.],\n",
      "        [ 2.,  6., 10., 14., 18.],\n",
      "        [ 3.,  7., 11., 15., 19.]])\n"
     ]
    }
   ],
   "source": [
    "print(A.t())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "Just as vectors generalize scalars, and matrices generalize vectors, we can actually build data structures with even more axes. <br>\n",
    "**`Tensors give us a generic way of discussing arrays with an arbitrary number of axes`.** <br>\n",
    "Vectors, for example, are first-order tensors, and matrices are second-order tensors. <br>\n",
    "Using tensors will become more important when we start working with images, which arrive as 3D data structures, with axes corresponding to the height, width, and the three (RGB) color channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스칼라가 벡터를 일반화하고, 매트릭스가 벡터를 일반화하는 것처럼, 실제로 더 많은 축을 가진 데이터 구조를 만들 수 있습니다. **`텐서는 임의의 축 수를 가진 배열을 논의하는 일반적인 방법을 제공합니다`.** 예를 들어, 벡터는 1차 텐서이며 매트릭스는 2차 텐서입니다. 이미지 작업을 시작할 때 텐서를 사용하는 것이 더 중요해질 것인데, 이미지는 높이, 너비 및 세 가지 (RGB) 색상 채널에 해당하는 축을 가진 3D 데이터 구조로 도착하기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = torch.Size([2, 3, 4])\n",
      "X = tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(24).reshape((2, 3, 4))\n",
    "print('X.shape =', X.shape)\n",
    "print('X =', X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic properties of tensor arithmetic\n",
    "\n",
    "Scalars, vectors, matrices, and tensors of any order have some nice properties that we will often rely on.<br>\n",
    "For example, as you might have noticed from the definition of an element-wise operation,\n",
    "given operands with the same shape,\n",
    "the result of any element-wise operation is a tensor of that same shape.<br>\n",
    "Another convenient property is that for all tensors, multiplication by a scalar\n",
    "produces a tensor of the same shape.  \n",
    "<span style=\"color:yellow\"> **In math, given two tensors $X$ and $Y$ with the same shape,\n",
    "$\\alpha X + Y$ has the same shape**</span>\n",
    "(numerical mathematicians call this the AXPY operation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스칼라, 벡터, 매트릭스 및 임의 차수의 텐서는 종종 의존할 몇 가지 유용한 특성을 가지고 있습니다.\n",
    "\n",
    "예를 들어, 원소별 연산의 정의에서 알 수 있듯이, 동일한 형태를 가진 피연산자가 주어지면 원소별 연산의 결과는 해당 형태의 텐서입니다.\n",
    "\n",
    "또 다른 편리한 특성은 모든 텐서에 대해 스칼라 곱셈이 동일한 모양의 텐서를 생성한다는 것입니다. **수학적으로 동일한 형태를 가진 두 텐서 $X$와 $Y$가 주어진 경우, $\\alpha X + Y$도 동일한 형태를 가집니다.** (수치 수학자들은 이를 AXPY 작업이라고 부릅니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "a = 2\n",
    "x = torch.ones(3)\n",
    "y = torch.zeros(3)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print((a * x).shape)\n",
    "print((a * x + y).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sums and means\n",
    "\n",
    "The next more sophisticated thing we can do with arbitrary tensors\n",
    "is to calculate the sum of their elements.  \n",
    "In mathematical notation, we express sums using the $\\sum$ symbol.  \n",
    "To express the sum of the elements in a vector $\\mathbf{u}$ of length $d$,\n",
    "we can write $\\sum_{i=1}^d u_i$. In code, we can just call `torch.sum()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임의의 텐서로 수행할 수 있는 다음 더 정교한 작업은 그 요소들의 합을 계산하는 것입니다.\n",
    "\n",
    "수학적 표기법에서 우리는 합을 $\\sum$ 기호를 사용하여 표현합니다.\n",
    "\n",
    "길이가 $d$인 벡터 $\\mathbf{u}$의 요소들의 합을 표현하기 위해 $\\sum_{i=1}^d u_i$라고 쓸 수 있습니다. 코드에서는 간단히 `torch.sum()`을 호출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(torch.sum(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can similarly express sums over the elements of tensors of arbitrary shape. For example, the sum of the elements of an $m \\times n$ matrix $A$ could be written $\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "tensor(190.)\n"
     ]
    }
   ],
   "source": [
    "print(A)\n",
    "print(torch.sum(A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related quantity is the *mean*, which is also called the *average*.\n",
    "We calculate the mean by dividing the sum by the total number of elements.\n",
    "With mathematical notation, we could write the average\n",
    "over a vector $\\mathbf{u}$ as $\\frac{1}{d} \\sum_{i=1}^{d} u_i$\n",
    "and the average over a matrix $A$ as  $\\frac{1}{n \\cdot m} \\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$.\n",
    "In code, we could just call ``torch.mean()`` on tensors of arbitrary shape:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "관련된 양은 *평균* 또는 *평균*이라고도 불립니다. 평균은 합계를 총 요소 수로 나누어 계산합니다. 수학적 표기법을 사용하면 벡터 $\\mathbf{u}$에 대한 평균을 $\\frac{1}{d} \\sum_{i=1}^{d} u_i$로 쓸 수 있으며, 매트릭스 $A$에 대한 평균은 $\\frac{1}{n \\cdot m} \\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$로 쓸 수 있습니다. 코드에서는 임의의 형태의 텐서에 대해 간단히 ``torch.mean()``을 호출할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.5000)\n",
      "tensor(9.5000)\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(A))\n",
    "print(torch.div(torch.sum(A), torch.numel(A)))\n",
    "# torch.numel() calculates number of elements in a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 60.,  30.,  10.,  30.],\n",
      "        [100.,  70.,  50.,  30.]])\n",
      "tensor(80.)\n",
      "tensor(80.)\n"
     ]
    }
   ],
   "source": [
    "s = torch.tensor([[60, 30, 10, 30], [100, 70, 50, 30]], dtype=torch.float32)\n",
    "\n",
    "print(s)\n",
    "print(torch.mean(s[:,0]))\n",
    "print(torch.mean(s,dim=0)[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot products\n",
    "\n",
    "So far, we have only performed element-wise operations, sums and averages. And if this was all we could do, linear algebra probably would not deserve its own chapter. However, one of the most fundamental operations is the dot product. Given two vectors $\\mathbf{u}$ and $\\mathbf{v}$, the dot product $\\mathbf{u}^T \\mathbf{v}$ is a sum over the products of the corresponding elements: $\\mathbf{u}^T \\mathbf{v} = \\sum_{i=1}^{d} u_i \\cdot v_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 우리는 원소별 연산, 합 및 평균만 수행했습니다. 이게 우리가 할 수 있는 모든 것이라면, 선형 대수학은 아마도 자체 챕터를 가치지 않을 것입니다. 그러나 가장 기본적인 작업 중 하나는 내적(dot product)입니다. 두 벡터 $\\mathbf{u}$와 $\\mathbf{v}$가 주어지면 내적 $\\mathbf{u}^T \\mathbf{v}$는 해당 요소들의 곱의 합입니다: $\\mathbf{u}^T \\mathbf{v} = \\sum_{i=1}^{d} u_i \\cdot v_i$입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.]) tensor([1., 1., 1., 1.]) tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4, dtype = torch.float32)\n",
    "y = torch.ones(4, dtype = torch.float32)\n",
    "print(x, y, torch.dot(x, y))\n",
    "\n",
    "\n",
    "#x, y = x.reshape(1,-1), y.reshape(-1,1)\n",
    "#print(x, y, torch.mm(x,y))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can express the dot product of two vectors ``torch.dot(x, y)`` equivalently by performing an element-wise multiplication and then a sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x * y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot products are useful in a wide range of contexts. For example, given a set of weights $\\mathbf{w}$, the weighted sum of some values ${u}$ could be expressed as the dot product $\\mathbf{u}^T \\mathbf{w}$. When the weights are non-negative and sum to one $\\left(\\sum_{i=1}^{d} {w_i} = 1\\right)$, the dot product expresses a *weighted average*. When two vectors each have length one (we will discuss what *length* means below in the section on norms), dot products can also capture the cosine of the angle between them.\n",
    "\n",
    "## Matrix-vector products\n",
    "\n",
    "Now that we know how to calculate dot products we can begin to understand matrix-vector products. Let's start off by visualizing a matrix $A$ and a column vector $\\mathbf{x}$.\n",
    "\n",
    "$$A=\\begin{pmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nm} \\\\\n",
    "\\end{pmatrix},\\quad\\mathbf{x}=\\begin{pmatrix}\n",
    " x_{1}  \\\\\n",
    " x_{2} \\\\\n",
    "\\vdots\\\\\n",
    " x_{m}\\\\\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "We can visualize the matrix in terms of its row vectors\n",
    "\n",
    "$$A=\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1} \\\\\n",
    "\\mathbf{a}^T_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^T_n \\\\\n",
    "\\end{pmatrix},$$\n",
    "\n",
    "where each $\\mathbf{a}^T_{i} \\in \\mathbb{R}^{m}$\n",
    "is a row vector representing the $i$-th row of the matrix $A$.\n",
    "\n",
    "Then the matrix vector product $\\mathbf{y} = A\\mathbf{x}$ is simply a column vector $\\mathbf{y} \\in \\mathbb{R}^n$ where each entry $y_i$ is the dot product $\\mathbf{a}^T_i \\mathbf{x}$.\n",
    "\n",
    "$$A\\mathbf{x}=\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1}  \\\\\n",
    "\\mathbf{a}^T_{2}  \\\\\n",
    " \\vdots  \\\\\n",
    "\\mathbf{a}^T_n \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    " x_{1}  \\\\\n",
    " x_{2} \\\\\n",
    "\\vdots\\\\\n",
    " x_{m}\\\\\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    " \\mathbf{a}^T_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^T_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^T_{n} \\mathbf{x}\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "<span style=\"color:yellow\"> **So you can think of multiplication by a matrix $A\\in \\mathbb{R}^{n \\times m}$ as a transformation that projects vectors from $\\mathbb{R}^{m}$ to $\\mathbb{R}^{n}$.** </span>\n",
    "\n",
    "These transformations turn out to be remarkably useful.  \n",
    "For example, we can represent rotations as multiplications by a square matrix.  \n",
    "\n",
    "<span style=\"color:yellow\"> **As we will see in subsequent chapters, we can also use matrix-vector products to describe the calculations of each layer in a neural network.**</span>\n",
    "\n",
    "Expressing matrix-vector products in code with ``tensor``, we use ``torch.mv()`` function.  \n",
    "When we call ``torch.mv(A, x)`` with a matrix ``A`` and a vector ``x``, PyTorch knows to perform a matrix-vector product.  \n",
    "<span style=\"color:yellow\"> **Note that the column dimension of ``A`` must be the same as the dimension of ``x``.** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내적은 다양한 상황에서 유용합니다. 예를 들어, 일련의 가중치 $\\mathbf{w}$가 주어진 경우 일부 값 ${u}$의 가중 합은 내적 $\\mathbf{u}^T \\mathbf{w}$로 표현할 수 있습니다. 가중치가 음수가 아니며 1로 합산되면 ($\\sum_{i=1}^{d} {w_i} = 1$), 내적은 *가중 평균*(weighted average)을 표현합니다. 두 벡터의 길이가 각각 1인 경우 (길이가 무엇을 의미하는지는 아래에서 정규화(norms) 섹션에서 설명합니다), 내적은 두 벡터 사이의 각도의 코사인도 포함할 수 있습니다.\n",
    "\n",
    "## 매트릭스-벡터 곱셈\n",
    "\n",
    "내적을 계산하는 방법을 알게 되었으므로 이제 매트릭스-벡터 곱셈을 이해할 수 있습니다. 매트릭스 $A$와 열 벡터 $\\mathbf{x}$를 시각화하여 시작해 보겠습니다.\n",
    "\n",
    "$$A=\\begin{pmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nm} \\\\\n",
    "\\end{pmatrix},\\quad\\mathbf{x}=\\begin{pmatrix}\n",
    " x_{1}  \\\\\n",
    " x_{2} \\\\\n",
    "\\vdots\\\\\n",
    " x_{m}\\\\\n",
    "\\end{pmatrix} $$\n",
    "\n",
    "매트릭스를 행 벡터로 시각화할 수 있습니다.\n",
    "\n",
    "$$A=\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1} \\\\\n",
    "\\mathbf{a}^T_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^T_n \\\\\n",
    "\\end{pmatrix},$$\n",
    "\n",
    "각 $\\mathbf{a}^T_{i} \\in \\mathbb{R}^{m}$는 매트릭스 $A$의 $i$-번째 행을 나타내는 행 벡터입니다.\n",
    "\n",
    "그런 다음 매트릭스-벡터 곱셈 $\\mathbf{y} = A\\mathbf{x}$은 간단히 각 항목 $y_i$가 내적 $\\mathbf{a}^T_i \\mathbf{x}$인 열 벡터 $\\mathbf{y} \\in \\mathbb{R}^n$입니다.\n",
    "\n",
    "$$A\\mathbf{x}=\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1}  \\\\\n",
    "\\mathbf{a}^T_{2}  \\\\\n",
    " \\vdots  \\\\\n",
    "\\mathbf{a}^T_n \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    " x_{1}  \\\\\n",
    " x_{2} \\\\\n",
    "\\vdots\\\\\n",
    " x_{m}\\\\\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    " \\mathbf{a}^T_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^T_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^T_{n} \\mathbf{x}\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "<span style=\"color:yellow\"> **그러므로 행렬 $A\\in \\mathbb{R}^{n \\times m}$에 의한 곱셈을 $\\mathbb{R}^{m}$에서 $\\mathbb{R}^{n}$로 벡터를 투영하는 변환으로 생각할 수 있습니다.** </span>\n",
    "\n",
    "이러한 변환이 매우 유용하다는 것이 밝혀졌습니다.  \n",
    "예를 들어, 제곱 매트릭스에 의한 곱셈으로 회전을 표현할 수 있습니다.\n",
    "\n",
    "<span style=\"color:yellow\"> **향후 챕터에서 볼 것처럼, 신경망의 각 레이어에서의 계산을 설명하는 데도 매트릭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "tensor([0., 1., 2., 3.])\n",
      "tensor([ 14.,  38.,  62.,  86., 110.])\n",
      "tensor(14.) tensor(38.) tensor(62.) tensor(86.) tensor(110.)\n",
      "tensor([[ 14.],\n",
      "        [ 38.],\n",
      "        [ 62.],\n",
      "        [ 86.],\n",
      "        [110.]])\n"
     ]
    }
   ],
   "source": [
    "print(A)\n",
    "print(x)\n",
    "print(torch.mv(A, x))\n",
    "print( torch.dot(A[0],x), torch.dot(A[1],x), torch.dot(A[2],x), torch.dot(A[3],x), torch.dot(A[4],x) )\n",
    "print(torch.mm(A,x.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A)\n",
    "\n",
    "print(x)\n",
    "\n",
    "# torch.mv 함수를 사용하여 A와 x 사이의 행렬-벡터 곱을 계산하고 출력합니다.\n",
    "# 결과는 크기가 5인 1차원 벡터로, A의 각 행과 x의 내적으로 계산됩니다.\n",
    "print(torch.mv(A, x))\n",
    "\n",
    "# torch.dot 함수를 사용하여 A의 각 행과 x 사이의 내적을 개별적으로 계산하고 출력합니다.\n",
    "# 결과는 각 행의 내적을 나타내는 5개의 스칼라 값으로 출력됩니다.\n",
    "print(torch.dot(A[0], x), torch.dot(A[1], x), torch.dot(A[2], x), torch.dot(A[3], x), torch.dot(A[4], x))\n",
    "\n",
    "# torch.mm 함수를 사용하여 A와 x를 행렬 곱셈합니다.\n",
    "# x는 (4,1) 모양의 열 벡터로 재형성됩니다.\n",
    "# 결과는 (5,1) 모양의 열 벡터로 출력됩니다.\n",
    "print(torch.mm(A, x.reshape(-1, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[10, 20, 30], [10, 70, 50]], dtype = torch.float32)\n",
    "w = torch.tensor([0.3, 0.2, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(42.)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(A[1],w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22., 42.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mv(A, w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-matrix multiplication\n",
    "\n",
    "If you have gotten the hang of dot products and matrix-vector multiplication, then matrix-matrix multiplications should be pretty straightforward.\n",
    "\n",
    "Say we have two matrices, $A \\in \\mathbb{R}^{n \\times k}$ and $B \\in \\mathbb{R}^{k \\times m}$:\n",
    "\n",
    "$$A=\\begin{pmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
    "\\end{pmatrix},\\quad\n",
    "B=\\begin{pmatrix}\n",
    " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "To produce the matrix product $C = AB$, it's easiest to think of $A$ in terms of its row vectors and $B$ in terms of its column vectors:\n",
    "\n",
    "$$A=\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1} \\\\\n",
    "\\mathbf{a}^T_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^T_n \\\\\n",
    "\\end{pmatrix},\n",
    "\\quad B=\\begin{pmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Note here that each row vector $\\mathbf{a}^T_{i}$ lies in $\\mathbb{R}^k$ and that each column vector $\\mathbf{b}_j$ also lies in $\\mathbb{R}^k$.\n",
    "\n",
    "Then to produce the matrix product $C \\in \\mathbb{R}^{n \\times m}$ we simply compute each entry $c_{ij}$ as the dot product $\\mathbf{a}^T_i \\mathbf{b}_j$.\n",
    "\n",
    "$$C = AB = \\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1} \\\\\n",
    "\\mathbf{a}^T_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^T_n \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "\\mathbf{a}^T_{1} \\mathbf{b}_1 & \\mathbf{a}^T_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^T_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^T_{2}\\mathbf{b}_1 & \\mathbf{a}^T_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^T_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^T_{n} \\mathbf{b}_1 & \\mathbf{a}^T_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^T_{n} \\mathbf{b}_m\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "You can think of the matrix-matrix multiplication $AB$ as simply performing $m$ matrix-vector products and stitching the results together to form an $n \\times m$ matrix.  \n",
    "<span style=\"color:yellow\"> **We can compute matrix-matrix products in PyTorch by using ``torch.mm()``.** </span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms\n",
    "\n",
    "Before we can start implementing models,\n",
    "there is one last concept we are going to introduce.\n",
    "Some of the most useful operators in linear algebra are norms.\n",
    "Informally, they tell us how big a vector or matrix is.\n",
    "We represent norms with the notation $\\|\\cdot\\|$.\n",
    "The $\\cdot$ in this expression is just a placeholder.\n",
    "For example, we would represent the norm of a vector $\\mathbf{x}$\n",
    "or matrix $A$ as $\\|\\mathbf{x}\\|$ or $\\|A\\|$, respectively.\n",
    "\n",
    "All norms must satisfy a handful of properties:\n",
    "\n",
    "1. $\\|\\alpha A\\| = |\\alpha| \\|A\\|$\n",
    "1. $\\|A + B\\| \\leq \\|A\\| + \\|B\\|$\n",
    "1. $\\|A\\| \\geq 0$\n",
    "1. If $\\forall {i,j}, a_{ij} = 0$, then $\\|A\\|=0$\n",
    "\n",
    "To put it in words, the first rule says\n",
    "that if we scale all the components of a matrix or vector\n",
    "by a constant factor $\\alpha$,\n",
    "its norm also scales by the *absolute value*\n",
    "of the same constant factor.\n",
    "The second rule is the familiar triangle inequality.\n",
    "The third rule simply says that the norm must be non-negative.\n",
    "That makes sense, in most contexts the smallest *size* for anything is 0.\n",
    "The final rule basically says that the smallest norm is achieved by a matrix or vector consisting of all zeros.\n",
    "It is possible to define a norm that gives zero norm to nonzero matrices,\n",
    "but you cannot give nonzero norm to zero matrices.\n",
    "That may seem like a mouthful, but if you digest it then you probably have grepped the important concepts here.\n",
    "\n",
    "If you remember Euclidean distances (think Pythagoras' theorem) from grade school,\n",
    "then non-negativity and the triangle inequality might ring a bell.\n",
    "You might notice that norms sound a lot like measures of distance.\n",
    "\n",
    "<span style=\"color:yellow\"> **In fact, the Euclidean distance $\\sqrt{x_1^2 + \\cdots + x_n^2}$ is a norm.**</span>\n",
    "\n",
    "\n",
    "Specifically it is the $\\ell_2$-norm.\n",
    "An analogous computation,\n",
    "performed over the entries of a matrix, e.g. $\\sqrt{\\sum_{i,j} a_{ij}^2}$,\n",
    "is called the Frobenius norm.\n",
    "\n",
    "<span style=\"color:yellow\"> **More often, in machine learning we work with the squared $\\ell_2$ norm (notated $\\ell_2^2$) because MSE is a common loss we should minimize.**</span>\n",
    "\n",
    "We also commonly work with the $\\ell_1$ norm.\n",
    "The $\\ell_1$ norm is simply the sum of the absolute values.\n",
    "It has the convenient property of placing less emphasis on outliers.\n",
    "\n",
    "To calculate the $\\ell_2$ norm, we can just call ``torch.norm()``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 노름 (Norms)\n",
    "\n",
    "모델을 구현하기 전에 소개할 마지막 개념이 하나 있습니다. 선형 대수학에서 가장 유용한 연산자 중 일부는 노름(norms)입니다. 비공식적으로, 노름은 벡터 또는 행렬의 크기를 나타냅니다. 노름은 $\\|\\cdot\\|$ 표기법으로 나타냅니다. 이 식에서 $\\cdot$은 단순히 자리 표시자입니다. 예를 들어, 벡터 $\\mathbf{x}$ 또는 행렬 $A$의 노름은 각각 $\\|\\mathbf{x}\\|$ 또는 $\\|A\\|$로 나타낼 수 있습니다.\n",
    "\n",
    "모든 노름은 몇 가지 속성을 만족해야 합니다:\n",
    "\n",
    "1. $\\|\\alpha A\\| = |\\alpha| \\|A\\|$\n",
    "2. $\\|A + B\\| \\leq \\|A\\| + \\|B\\|$\n",
    "3. $\\|A\\| \\geq 0$\n",
    "4. 만약 $\\forall {i,j}, a_{ij} = 0$이면, $\\|A\\|=0$\n",
    "\n",
    "이를 간략히 설명하면, 첫 번째 규칙은 행렬 또는 벡터의 모든 구성 요소를 상수 배율 $\\alpha$로 확장하면 그 노름도 *동일한 상수의 절댓값* 만큼 확장된다는 것을 의미합니다. 두 번째 규칙은 익숙한 삼각부등식입니다. 세 번째 규칙은 단순히 노름은 음수일 수 없다는 것을 나타냅니다. 대부분의 상황에서 무엇이든 가장 작은 *크기*는 0입니다. 마지막 규칙은 사실상 모든 원소가 0인 행렬이나 벡터는 노름이 0이어야 한다는 것을 나타냅니다. 0이 아닌 행렬에 0 노름을 할당할 수 있는 노름을 정의할 수 있지만 0인 행렬에 0이 아닌 노름을 할당할 수는 없습니다. 이것은 어렵게 들릴 수 있지만, 이를 이해하면 중요한 개념을 이해한 것입니다.\n",
    "\n",
    "학창 시절 유클리드 거리(피타고라스의 정리를 생각해보세요)를 기억한다면, 비음성성과 삼각부등식이 익숙할 것입니다. 노름은 거리 측정 방법과 매우 유사하다는 것을 알 수 있습니다.\n",
    "\n",
    "<span style=\"color:yellow\"> **실제로 유클리드 거리 $\\sqrt{x_1^2 + \\cdots + x_n^2}$ 는 노름입니다.**</span>\n",
    "\n",
    "구체적으로 이것은 $\\ell_2$ 노름입니다. 행렬의 항목에 대한 유사한 계산, 예를 들어 $\\sqrt{\\sum_{i,j} a_{ij}^2}$ 은 프로베니우스 노름이라고 합니다.\n",
    "\n",
    "<span style=\"color:yellow\"> **머신 러닝에서는 주로 제곱 $\\ell_2$ 노름 (표기: $\\ell_2^2$)을 사용하는데, 이는 MSE(평균 제곱 오차)가 최소화해야 하는 흔한 손실 함수이기 때문입니다.**</span>\n",
    "\n",
    "또한 주로 $\\ell_1$ 노름을 사용합니다. $\\ell_1$ 노름은 단순히 절댓값의 합입니다. 이것은 이상값(outlier)에 덜 중점을 두는 편리한 특성을 가지고 있습니다.\n",
    "\n",
    "$\\ell_2$ 노름을 계산하려면 \"torch.norm()\"을 호출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.])\n",
      "tensor(3.7417)\n",
      "tensor(3.7417)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print( torch.sqrt(torch.sum(x**2)) )\n",
    "print( torch.norm(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.,  1.])\n",
      "tensor(1.4142)\n",
      "tensor(1.4142)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-1, 1], dtype = torch.float32)\n",
    "print(x)\n",
    "print( torch.sqrt(torch.sum(x**2)) )\n",
    "print( torch.norm(x) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the L1-norm we can simply perform the absolute value and then sum over the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.abs(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:yellow\">Lab: Solving A System of Equations</span>\n",
    "\n",
    "$x_1 + x_2 + x_3 = 0$\n",
    "\n",
    "$2x_1 + x_2 + x_3 = 1$\n",
    "\n",
    "$x_1 + x_2 - x_3 = 4$\n",
    "\n",
    "can be rewritten by a matrix form\n",
    "\n",
    "$\\begin{bmatrix*}[r]\n",
    "   1 & 1 & 1\\\\\n",
    "   2 & 1 & 1\\\\\n",
    "   1 & 1 & -1    \n",
    "\\end{bmatrix*}\\begin{bmatrix*}[r]\n",
    "   x_1\\\\\n",
    "   x_2\\\\\n",
    "   x_3\n",
    "\\end{bmatrix*} = \\begin{bmatrix*}[r]\n",
    "   0\\\\\n",
    "   1\\\\\n",
    "   4\n",
    "\\end{bmatrix*}$.\n",
    "\n",
    "Then, $\\mathbf{x}^{T}=[x_1~ x_2~ x_3]$ can be calculated by\n",
    "\n",
    "$\\begin{bmatrix*}[r]\n",
    "   x_1\\\\\n",
    "   x_2\\\\\n",
    "   x_3\n",
    "\\end{bmatrix*} = \\begin{bmatrix*}[r]\n",
    "   1 & 1 & 1\\\\\n",
    "   2 & 1 & 1\\\\\n",
    "   1 & 1 & -1    \n",
    "\\end{bmatrix*}^{-1}\\begin{bmatrix*}[r]\n",
    "   0\\\\\n",
    "   1\\\\\n",
    "   4\n",
    "\\end{bmatrix*}$.\n",
    "\n",
    "<span style=\"color:yellow\"> **Solve this by using torch.mm or torch.mv along with torch.inverse.** </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[1,1,1],[2,1,1],[1,1,-1]], dtype = torch.float32)\n",
    "b = torch.tensor([0,1,4], dtype = torch.float32)\n",
    "A_inv = torch.inverse(A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using torch.dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(1.) tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.dot(A_inv[0], b)\n",
    "x2 = torch.dot(A_inv[1], b)\n",
    "x3 = torch.dot(A_inv[2], b)\n",
    "print(x1, x2, x3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using torch.mv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  1., -2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.mv(A_inv, b)\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using torch.mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.],\n",
      "        [ 1.],\n",
      "        [-2.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.mm(A_inv, b.reshape(-1,1))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9970af6c6272f912aea970d3aa30162bd36dbb3ebdad53a40420a930d21aa1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
