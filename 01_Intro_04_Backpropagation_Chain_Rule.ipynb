{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisiting Automatic Differentiation\n",
    "\n",
    "In machine learning, we *train* models, updating them successively so that they get better and better as they see more and more data. Usually, *getting better* means minimizing a *loss function*, a score that answers the question \"how *bad* is our model?\" With neural networks, we typically choose loss functions that are differentiable with respect to our parameters.\n",
    "Put simply, this means that for each of the model's parameters, we can determine how much *increasing* or *decreasing* it might affect the loss. While the calculations for taking these derivatives are straightforward, requiring only some basic calculus, for complex models, working out the updates by hand can be a pain (and often error-prone)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자동 미분 재방문\n",
    "\n",
    "기계 학습에서 모델을 *훈련*하여 계속해서 업데이트하여 더 많은 데이터를 보면서 점점 더 나아지게 만듭니다. 일반적으로 *더 나아지는 것*은 *손실 함수*를 최소화하는 것을 의미하며, 이는 \"우리 모델은 얼마나 *나쁜*가?\"라는 질문에 답하는 점수입니다. 신경망에서 우리는 일반적으로 매개변수에 대한 미분 가능한 손실 함수를 선택합니다. 간단히 말하면, 이는 모델의 각 매개변수에 대해 손실에 얼마나 *증가* 또는 *감소*하는 영향을 결정할 수 있는 것을 의미합니다. 이러한 미분값을 계산하는 계산은 기본적인 미적분만 필요로 하지만 복잡한 모델의 경우 수동으로 업데이트를 계산하는 작업은 고통스럽고 (때로는 오류가 발생할 수 있음) 어렵습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8945],\n",
      "        [-0.3214],\n",
      "        [-0.7774],\n",
      "        [ 0.9983]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(4, dtype=torch.float32).reshape((4, 1))\n",
    "x.requires_grad=True\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.0086]], grad_fn=<MulBackward0>)\n",
      "x.grad: tensor([[ 3.5781],\n",
      "        [-1.2854],\n",
      "        [-3.1094],\n",
      "        [ 3.9932]])\n",
      "x.grad_fn: None\n",
      "y.grad_fn: <MulBackward0 object at 0x7fa58846c760>\n"
     ]
    }
   ],
   "source": [
    "y = 2*torch.mm(x.t(),x) # y = 2* ((w,x,y,z)의 전치행렬*(w,x,y,z)) => mm=행렬곱셉 => y = 2* (w^2 + x^2 + y^2 + z^2)\n",
    "print(y)\n",
    "y.backward()\n",
    "print(\"x.grad:\", x.grad)\n",
    "print(\"x.grad_fn:\", x.grad_fn)\n",
    "print(\"y.grad_fn:\", y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Back Propagation Using Chain Rule`\n",
    "\n",
    "*Caution: This part is tricky and not necessary to understanding subsequent sections. That said, it is needed if you want to build new layers from scratch. You can skip this on a first read.*\n",
    "\n",
    "Sometimes when we call the backward method, e.g. `y.backward()`, where\n",
    "`y` is a function of `x` we are just interested in the derivative of\n",
    "`y` with respect to `x`. Mathematicians write this as\n",
    "$\\frac{dy(x)}{dx}$. At other times, we may be interested in the\n",
    "gradient of `z` with respect to `x`, where `z` is a function of `y`,\n",
    "which in turn, is a function of `x`. That is, we are interested in\n",
    "$\\frac{d}{dx} z(y(x))$. Recall that by the chain rule\n",
    "\n",
    "$$\\frac{d}{dx} z(y(x)) = \\frac{dz(y)}{dy} \\frac{dy(x)}{dx}.$$\n",
    "\n",
    "So, when ``y`` is part of a larger function ``z`` and we want ``x.grad`` to store $\\frac{dz}{dx}$, we can pass in the *head gradient* $\\frac{dz}{dy}$ as an input to ``backward()``. The default argument is ``torch.ones_like(y)``. See [Wikipedia](https://en.wikipedia.org/wiki/Chain_rule) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 체인 룰을 사용한 역전파\n",
    "\n",
    "*주의: 이 부분은 까다롭고 후속 섹션을 이해하는 데 필요하지 않습니다. 그럼에도 불구하고 처음 읽을 때는 이해할 필요가 있습니다. 이 내용은 처음부터 새로운 레이어를 만들고자 할 때 필요합니다. 처음에는 이 부분을 건너 뛸 수 있습니다.*\n",
    "\n",
    "가끔 우리는 역전파 메서드를 호출할 때, 예를 들어 `y.backward()`와 같이 호출할 때, `y`가 `x`의 함수인 도함수, 즉 $\\frac{dy(x)}{dx}$에만 관심이 있을 수 있습니다. 다른 경우에는 `z`가 `y`의 함수이며 이는 `x`의 함수일 수 있습니다. 즉, $\\frac{d}{dx} z(y(x))$에 관심이 있을 수 있습니다. 체인 룰에 따라 다음과 같이 표현할 수 있습니다.\n",
    "\n",
    "$$\\frac{d}{dx} z(y(x)) = \\frac{dz(y)}{dy} \\frac{dy(x)}{dx}.$$\n",
    "\n",
    "따라서 `y`가 더 큰 함수 `z`의 일부이고 `x.grad`가 $\\frac{dz}{dx}$를 저장하려고 할 때, *헤드 그래디언트*(head gradient)를 `backward()`에 입력으로 전달할 수 있습니다. 기본 인수는 `torch.ones_like(y)`입니다. 자세한 내용은 [Wikipedia](https://en.wikipedia.org/wiki/Chain_rule)를 참조하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000],\n",
      "        [4.0000],\n",
      "        [0.8000],\n",
      "        [0.1200]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[0.],[1.],[2.],[3.]], requires_grad=True)\n",
    "y = x * 2\n",
    "z = y * x\n",
    "\n",
    "head_gradient = torch.tensor([[10], [1.], [.1], [.01]])\n",
    "#head_gradient = torch.tensor([[1.], [1.], [1.], [1.]])\n",
    "z.backward(head_gradient)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph\n",
    "\n",
    "Let’s assume we want to perform the following set of operations to get our result r:\n",
    "\n",
    "<img src=\"img/comp_graph.jpeg\" width=700>\n",
    "\n",
    "$$\n",
    "r=z^2(x^2+y)^2\n",
    "$$\n",
    "\n",
    "<span style=\"color:yellow\"> $x, y, z$ are leaf variables!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass (Propagation)\n",
    "\n",
    "For example,  \n",
    "If $x=1, y=2, z=4$, the final output is $r=144$.\n",
    "\n",
    "<img src=\"img/forward_pass.jpeg\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass (Back Propagation)\n",
    "\n",
    "To calculate gradients with regards to each of 3 variables we have to calculate partial derivatives at each node in the graph (local gradients).  \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial r}{\\partial w} &= \\frac{\\partial w^2}{\\partial w} &= 2w \\\\\n",
    "\\frac{\\partial w}{\\partial v} &= \\frac{\\partial zv}{\\partial v} &= z \\\\\n",
    "\\frac{\\partial w}{\\partial z} &= \\frac{\\partial zv}{\\partial z} &= v \\\\\n",
    "\\frac{\\partial v}{\\partial u} &= \\frac{\\partial (u+y)}{\\partial u} &= 1 \\\\\n",
    "\\frac{\\partial v}{\\partial y} &= \\frac{\\partial (u+y)}{\\partial y} &= 1 \\\\\n",
    "\\frac{\\partial u}{\\partial x} &= \\frac{\\partial x^2}{\\partial x} &= 2x\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\frac{\\partial r}{\\partial z} = \\frac{\\partial r}{\\partial w}\\frac{\\partial w}{\\partial z} = 2wv = 72\n",
    "$<br><br>\n",
    "$\n",
    "\\frac{\\partial r}{\\partial y} = \\frac{\\partial r}{\\partial w}\\frac{\\partial w}{\\partial v}\\frac{\\partial v}{\\partial y} = 2wz\\cdot 1 = 96\n",
    "$<br><br>\n",
    "$\n",
    "\\frac{\\partial r}{\\partial x} = \\frac{\\partial r}{\\partial w}\\frac{\\partial w}{\\partial v}\\frac{\\partial v}{\\partial u}\\frac{\\partial u}{\\partial x} = 2wz\\cdot 2x = 4wz=192\n",
    "$<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 역전파 (Backward Pass)\n",
    "\n",
    "3개의 변수에 대한 그래디언트를 계산하려면 그래프의 각 노드에서 부분 미분값, 즉 지역 그래디언트를 계산해야 합니다.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial r}{\\partial w} &= \\frac{\\partial w^2}{\\partial w} &= 2w \\\\\n",
    "\\frac{\\partial w}{\\partial v} &= \\frac{\\partial zv}{\\partial v} &= z \\\\\n",
    "\\frac{\\partial w}{\\partial z} &= \\frac{\\partial zv}{\\partial z} &= v \\\\\n",
    "\\frac{\\partial v}{\\partial u} &= \\frac{\\partial (u+y)}{\\partial u} &= 1 \\\\\n",
    "\\frac{\\partial v}{\\partial y} &= \\frac{\\partial (u+y)}{\\partial y} &= 1 \\\\\n",
    "\\frac{\\partial u}{\\partial x} &= \\frac{\\partial x^2}{\\partial x} &= 2x\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\frac{\\partial r}{\\partial z} = \\frac{\\partial r}{\\partial w}\\frac{\\partial w}{\\partial z} = 2wv = 72\n",
    "$<br><br>\n",
    "$\n",
    "\\frac{\\partial r}{\\partial y} = \\frac{\\partial r}{\\partial w}\\frac{\\partial w}{\\partial v}\\frac{\\partial v}{\\partial y} = 2wz\\cdot 1 = 96\n",
    "$<br><br>\n",
    "$\n",
    "\\frac{\\partial r}{\\partial x} = \\frac{\\partial r}{\\partial w}\\frac{\\partial w}{\\partial v}\\frac{\\partial v}{\\partial u}\\frac{\\partial u}{\\partial x} = 2wz\\cdot 2x = 4wz=192\n",
    "$<br><br>\n",
    "\n",
    "이렇게 하여 `r`을 각 변수에 대한 미분값을 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 144.0\n",
      "dr/dw = None = 24.0\n",
      "dr/dv = None\n",
      "dr/du = None\n",
      "\n",
      "dr/dx = 192.0\n",
      "dr/dy = 96.0\n",
      "dr/dz = 72.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pj/5xhy0s2d6bx728d6w_6bwp1w0000gn/T/ipykernel_2247/470171788.py:15: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:491.)\n",
      "  print(f'dr/dw = {w.grad} = {2*w}')\n",
      "/var/folders/pj/5xhy0s2d6bx728d6w_6bwp1w0000gn/T/ipykernel_2247/470171788.py:16: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:491.)\n",
      "  print(f'dr/dv = {v.grad}')\n",
      "/var/folders/pj/5xhy0s2d6bx728d6w_6bwp1w0000gn/T/ipykernel_2247/470171788.py:17: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:491.)\n",
      "  print(f'dr/du = {u.grad}\\n')\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "z = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "# forward pass\n",
    "u = x**2\n",
    "v = u+y\n",
    "w = z*v\n",
    "r = w**2\n",
    "\n",
    "\n",
    "print(f'r = {r}')\n",
    "# backward pass\n",
    "r.backward()\n",
    "print(f'dr/dw = {w.grad} = {2*w}')\n",
    "print(f'dr/dv = {v.grad}')\n",
    "print(f'dr/du = {u.grad}\\n')\n",
    "print(f'dr/dx = {x.grad}')\n",
    "print(f'dr/dy = {y.grad}')\n",
    "print(f'dr/dz = {z.grad}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 25.0\n",
      "dr/dw = None = 24.0\n",
      "dr/dv = None\n",
      "dr/du = None\n",
      "\n",
      "dr/dx = 12.0\n",
      "dr/dy = 6.0\n",
      "dr/dz = 8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pj/5xhy0s2d6bx728d6w_6bwp1w0000gn/T/ipykernel_42322/4218990450.py:11: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:491.)\n",
      "  print(f'dr/dw = {w.grad} = {2*w}')\n",
      "/var/folders/pj/5xhy0s2d6bx728d6w_6bwp1w0000gn/T/ipykernel_42322/4218990450.py:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:491.)\n",
      "  print(f'dr/dv = {v.grad}')\n",
      "/var/folders/pj/5xhy0s2d6bx728d6w_6bwp1w0000gn/T/ipykernel_42322/4218990450.py:13: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:491.)\n",
      "  print(f'dr/du = {u.grad}\\n')\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "z = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "r = (z**2)+((x**2+y)**2)\n",
    "\n",
    "\n",
    "print(f'r = {r}')\n",
    "# backward pass\n",
    "r.backward()\n",
    "print(f'dr/dw = {w.grad} = {2*w}')\n",
    "print(f'dr/dv = {v.grad}')\n",
    "print(f'dr/du = {u.grad}\\n')\n",
    "\n",
    "print(f'dr/dx = {x.grad}')\n",
    "print(f'dr/dy = {y.grad}')\n",
    "print(f'dr/dz = {z.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:yellow\"> Lab : Compute gradients of $r$ w.r.t. $(x, y, z)$ at (1, 2, 3)</span>\n",
    "\n",
    "The network output $r$ is given by:\n",
    "\n",
    "$$\n",
    "r=(x+y)^3z^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 243.0\n",
      "dr/dw = None = 24.0\n",
      "dr/dv = None\n",
      "dr/du = None\n",
      "\n",
      "dr/dx = 243.0\n",
      "dr/dy = 243.0\n",
      "dr/dz = 162.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pj/5xhy0s2d6bx728d6w_6bwp1w0000gn/T/ipykernel_2247/2817604518.py:17: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:491.)\n",
      "  print(f'dr/dw = {w.grad} = {2*w}')\n",
      "/var/folders/pj/5xhy0s2d6bx728d6w_6bwp1w0000gn/T/ipykernel_2247/2817604518.py:18: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:491.)\n",
      "  print(f'dr/dv = {v.grad}')\n",
      "/var/folders/pj/5xhy0s2d6bx728d6w_6bwp1w0000gn/T/ipykernel_2247/2817604518.py:19: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:491.)\n",
      "  print(f'dr/du = {u.grad}\\n')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "z = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "u = (x + y)**3\n",
    "v = u * z**2\n",
    "r = v\n",
    "\n",
    "print(f'r = {r}')\n",
    "\n",
    "# backward pass\n",
    "r.backward()\n",
    "\n",
    "print(f'dr/dw = {w.grad} = {2*w}')\n",
    "print(f'dr/dv = {v.grad}')\n",
    "print(f'dr/du = {u.grad}\\n')\n",
    "\n",
    "print(f'dr/dx = {x.grad}')\n",
    "print(f'dr/dy = {y.grad}')\n",
    "print(f'dr/dz = {z.grad}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf5d689ac1d312cef1fba9545d41ab0482d31686d08c5b391b55a4a6eb2b95c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
