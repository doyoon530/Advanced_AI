{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weaknesses of Linear Layers\n",
    "\n",
    "Linear layer is a `fully connected layer` meaning that every neuron in a given layer is connected to every neuron in the next layer.  \n",
    "This has two key implications:\n",
    "- It results in a `LOT of parameters`.\n",
    "- `The order of our features doesn’t matter.`\n",
    "\n",
    "Consider the simple image and fully connected network below:\n",
    "\n",
    "선형 계층(Linear layer)은 '완전 연결 계층(fully connected layer)'으로, 한 계층의 모든 뉴런이 다음 계층의 모든 뉴런과 연결됩니다. 이러한 특성에는 두 가지 주요한 영향이 있습니다.\n",
    "\n",
    "많은 파라미터 생성: 완전 연결 계층은 많은 파라미터를 생성합니다. 각 뉴런 간의 모든 연결은 학습해야 하는 가중치로 이어지기 때문에, 입력 및 출력의 크기에 따라 파라미터 수가 기하급수적으로 증가할 수 있습니다.\n",
    "\n",
    "특징의 순서 무시: 완전 연결 계층은 입력 특성의 순서를 고려하지 않습니다. 각 입력 특성이 모든 뉴런에 동등하게 전달되므로, 특성들 간의 위치나 순서적인 관계가 고려되지 않고 모든 입력이 동일하게 처리됩니다.\n",
    "\n",
    "간단한 이미지와 완전 연결 네트워크를 고려하여 이러한 선형 계층의 한계를 이해해보겠습니다.\n",
    "\n",
    "<img src=\"img/fully_connected_parameters.png\" width=700>\n",
    "\n",
    "<span style=\"color:red;font-size:26px\"> What happens for 256x256 image? <br>It is not feasible to process larger images with linear layers!!</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions for Images\n",
    "In a convolutional layer, an input array and a correlation kernel array are combined to produce an output array through a cross-correlation operation.  \n",
    "In our example, the input is a two-dimensional array with a shape of $4 \\times 4$ or (4, 4).  \n",
    "The height and width of the kernel array are both 2.  \n",
    "Common names for this array in the deep learning research community include `*kernel* and *filter*`.  \n",
    "The shape of the kernel window (also known as the convolution window) is given precisely by the height and width of the kernel\n",
    "\n",
    "합성곱(Convolutional) 계층에서는 입력 배열과 상관 관계(correlation) 커널 배열을 교차 상관(correlation) 연산을 통해 출력 배열을 생성합니다.\n",
    "우리의 예에서, 입력은 $4 \\times 4$ 또는 (4, 4) 모양의 이차원 배열입니다.\n",
    "커널 배열의 높이와 너비는 모두 2입니다.\n",
    "딥 러닝 연구 커뮤니티에서 이러한 배열을 일반적으로 *커널* 또는 *필터*라고 부릅니다.\n",
    "커널 창의 모양(합성곱 윈도우로도 알려짐)은 커널의 높이와 너비로 정확하게 결정됩니다.\n",
    "<span style=\"color:yellow;font-size:26\">($2 \\times 2$).\n",
    "\n",
    "\n",
    "<img src=\"img/cnn.gif\" width=700> <br>\n",
    "\n",
    "\n",
    "<img src=\"img/correlation.svg\" width=700>\n",
    "\n",
    "\n",
    "The shaded portions are the first output element and the input and kernel array elements used in its computation: $0\\times0+1\\times1+3\\times2+4\\times3=19$. \n",
    "\n",
    "Here, the output array has a height of 2 and width of 2 and the four elements are derived from the two-dimensional cross-correlation operation:\n",
    "\n",
    "음영 처리된 부분은 첫 번째 출력 요소와 해당 계산에 사용된 입력 및 커널 배열 요소입니다: $0\\times0+1\\times1+3\\times2+4\\times3=19$.\n",
    "\n",
    "여기서 출력 배열은 높이가 2이고 너비가 2이며, 이 네 요소는 이차원 교차 상관(correlation) 연산에서 파생됩니다:\n",
    "\n",
    "$$\n",
    "0\\times0+1\\times1+3\\times2+4\\times3=19,\\\\\n",
    "1\\times0+2\\times1+4\\times2+5\\times3=25,\\\\\n",
    "3\\times0+4\\times1+6\\times2+7\\times3=37,\\\\\n",
    "4\\times0+5\\times1+7\\times2+8\\times3=43.\n",
    "$$\n",
    "\n",
    "We implement the above process in the `corr2d` function.\n",
    "It accepts the input array `X` with the kernel array `K`\n",
    "and outputs the array `Y`.\n",
    "\n",
    "우리는 위의 과정을 corr2d 함수로 구현합니다.\n",
    "이 함수는 입력 배열 X와 커널 배열 K를 입력으로 받아 배열 Y를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def corr2d(X, K):\n",
    "    # 커널의 높이와 너비\n",
    "    h, w = K.shape\n",
    "    # 출력 배열 초기화\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "\n",
    "    # 2D 교차 상관 연산을 위한 반복문\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            # 교차 상관 연산 수행 및 결과 저장\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y\n",
    "\n",
    "# 입력 배열과 커널 배열 정의\n",
    "X = torch.Tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
    "K = torch.Tensor([[0, 1], [2, 3]])\n",
    "\n",
    "# 예제로 함수 호출 및 실행\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layers of Pytorch\n",
    "\n",
    "A convolutional layer cross-correlates the input and kernels\n",
    "and adds a scalar bias to produce an output.\n",
    "The parameters of the convolutional layer\n",
    "are precisely the values that constitute the kernel and the scalar bias.\n",
    "When training the models based on convolutional layers,\n",
    "we typically initialize the kernels randomly,\n",
    "just as we would with a fully-connected layer.\n",
    "\n",
    "We are now ready to implement a two-dimensional convolutional layer\n",
    "based on the `corr2d` function defined above.\n",
    "In the `__init__` constructor function,\n",
    "we declare `weight` and `bias` as the two model parameters.\n",
    "The forward computation function `forward`\n",
    "calls the `corr2d` function and adds the bias.\n",
    "As with $h \\times w$ cross-correlation\n",
    "we also refer to convolutional layers\n",
    "as $h \\times w$ convolutions.\n",
    "\n",
    "합성곱(convolutional) 계층은 입력과 커널을 교차 상관하고, 스칼라 편향을 추가하여 출력을 생성합니다. 합성곱 계층의 매개변수는 정확히 커널과 스칼라 편향으로 이루어진 값들입니다. 합성곱 계층을 기반으로 하는 모델을 훈련할 때, 일반적으로 커널을 무작위로 초기화합니다. 이는 완전 연결 계층에서의 초기화와 유사합니다.\n",
    "\n",
    "이제 앞서 정의한 corr2d 함수를 기반으로 2차원 합성곱(convolutional) 계층을 구현할 준비가 되었습니다. __init__ 생성자 함수에서 weight와 bias를 두 개의 모델 매개변수로 선언합니다. 순전파 연산 함수인 forward는 corr2d 함수를 호출하고 편향을 더합니다. $h \\times w$ 교차 상관과 마찬가지로 우리는 합성곱 계층을 $h \\times w$ 합성곱으로도 부릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "70"
    }
   },
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size, **kwargs):\n",
    "        super(Conv2D, self).__init__(**kwargs)\n",
    "        # 커널과 편향 초기화\n",
    "        self.weight = torch.rand(kernel_size, dtype=torch.float32, requires_grad=True)\n",
    "        self.bias = torch.zeros((1,), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        # corr2d 함수와 편향을 더한 결과 반환\n",
    "        return corr2d(x, self.weight) + self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Convolution vs Dilated Convolution\n",
    "<table>\n",
    "  <tbody><tr>\n",
    "    <td><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"./img/no_padding_no_strides.gif\"><img width=\"300px\" src=\"./img/no_padding_no_strides.gif\" data-animated-image=\"\" style=\"max-width: 100%;\"></a></td>\n",
    "    <td><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"./img/arbitrary_padding_no_strides.gif\"><img width=\"300px\" src=\"./img/arbitrary_padding_no_strides.gif\" data-animated-image=\"\" style=\"max-width: 100%;\"></a></td>\n",
    "    <td><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"./img/same_padding_no_strides.gif\"><img width=\"300px\" src=\"./img/same_padding_no_strides.gif\" data-animated-image=\"\" style=\"max-width: 100%;\"></a></td>\n",
    "    <td><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"./img/full_padding_no_strides.gif\"><img width=\"300px\" src=\"./img/full_padding_no_strides.gif\" data-animated-image=\"\" style=\"max-width: 100%;\"></a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, no strides</td>\n",
    "    <td>Arbitrary padding, no strides</td>\n",
    "    <td>Half padding, no strides</td>\n",
    "    <td>Full padding, no strides</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"./img/no_padding_strides.gif\"><img width=\"300px\" src=\"./img/no_padding_strides.gif\" data-animated-image=\"\" style=\"max-width: 100%;\"></a></td>\n",
    "    <td><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"./img/padding_strides.gif\"><img width=\"300px\" src=\"./img/padding_strides.gif\" data-animated-image=\"\" style=\"max-width: 100%;\"></a></td>\n",
    "    <td><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"./img/padding_strides_odd.gif\"><img width=\"300px\" src=\"./img/padding_strides_odd.gif\" data-animated-image=\"\" style=\"max-width: 100%;\"></a></td>\n",
    "    <td><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"./img/conv_dilated.gif\"><img width=\"300px\" src=\"./img/conv_dilated.gif\" data-animated-image=\"\" style=\"max-width: 100%;\"></a></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, strides</td>\n",
    "    <td>Padding, strides</td>\n",
    "    <td>Padding, strides (odd)</td>\n",
    "    <td><span style=\"color:yellow\"> Dilated Convolution</td>\n",
    "  </tr>\n",
    "</tbody></table>\n",
    "\n",
    "- Kernel Size : kernel size는 convolution의 시야(view)를 결정. 보통 2D에서 3x3 사용.\n",
    "- Stride : 이미지를 횡단할 때 커널의 스텝 사이즈. 기본값은 1이지만 보통 Max Pooling과 비슷하게 이미지를 다운샘플링하기 위해 2도 사용.\n",
    "- Padding : Padding은 샘플 테두리 처리 방법. 패딩된 Convolution은 input과 동일한 output 차원을 유지하는 반면, 패딩되지 않은 Convolution은 커널이 1보다 큰 경우 테두리의 일부가 사라짐.\n",
    "- Input & Output Channels : Convolution layer는 Input 채널의 특정 수(I)를 받아 output 채널의 특정 수(O)로 계산.\n",
    "- The number of Parameters: IxOxK로 계산. K는 커널의 수."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output size of 2D-Convolution Layer\n",
    "`Conv2d with Stride=2 and padding`=$\\frac{kernel}{2}$ means `downsampling` by $\\frac{1}{2}$.\n",
    "\n",
    "<img src=\"./img/conv_size.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 32, 46]) torch.Size([1, 10, 16, 23]) [16,23]\n",
      "torch.Size([1, 10, 8, 12])\n",
      "torch.Size([1, 10, 4, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# 입력 및 출력 채널, dilation, 커널 크기, 스트라이드, 패딩, 이미지의 높이와 너비 정의\n",
    "in_channels = 1\n",
    "out_channels = 10\n",
    "dilation = (1, 1)\n",
    "kernel_size = (5, 5)\n",
    "stride = (2, 2)\n",
    "padding = (2, 2)\n",
    "h, w = 32, 46\n",
    "\n",
    "# 합성곱 연산 후 출력 크기 계산 함수 정의\n",
    "def cal_conv_out(h, padding, kernel_size, stride, dilation=1):\n",
    "    size = int(np.floor((h + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1))\n",
    "    return size \n",
    "\n",
    "# Conv2d 레이어 초기화 및 입력 데이터 생성\n",
    "conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "x = torch.ones((1, in_channels, h, w))\n",
    "y = conv(x)\n",
    "\n",
    "# Conv2d 레이어로 계산된 출력 크기\n",
    "output_h = cal_conv_out(h, padding[0], kernel_size[0], stride[0], dilation[0])\n",
    "output_w = cal_conv_out(w, padding[1], kernel_size[1], stride[1], dilation[1])\n",
    "print(x.shape, y.shape, f'[{output_h},{output_w}]')\n",
    "\n",
    "# Average pooling 적용 후 출력 크기\n",
    "pool = torch.nn.AvgPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "y = pool(y)\n",
    "print(y.shape)\n",
    "\n",
    "# 두 번째 Conv2d 레이어 적용 후 출력 크기\n",
    "conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation)\n",
    "y = conv2(y)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10, 14, 14])\n",
      "torch.Size([10, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 입력 텐서 생성: 배치 크기 10, 채널 1, 높이 28, 너비 28인 텐서 생성\n",
    "x = torch.ones((10, 1, 28, 28))\n",
    "\n",
    "# Convolutional 레이어 생성: 입력 채널 1, 출력 채널 10, 커널 크기 5x5, 스트라이드 2, 패딩 2\n",
    "conv2d = nn.Conv2d(1, 10, kernel_size=5, stride=2, padding=2)\n",
    "\n",
    "# Convolutional 레이어를 통과한 출력\n",
    "y = conv2d(x)\n",
    "print(y.shape)\n",
    "\n",
    "# Transposed Convolutional 레이어 생성: 입력 채널 10, 출력 채널 1, 커널 크기 5x5, 스트라이드 2, 패딩 2, 출력 패딩 1\n",
    "convTrans2d = nn.ConvTranspose2d(10, 1, kernel_size=5, stride=2, padding=2, output_padding=1)\n",
    "\n",
    "# Transposed Convolutional 레이어를 통과한 출력\n",
    "y = convTrans2d(y)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposed Convolutional Layers of Pytorch\n",
    "\n",
    "<img src=\"./img/convTransposed2D.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ConvTranspose2d with Stride=2 and padding`=$\\frac{kernel}{2}$ means `upsampling` by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 16, 23]) torch.Size([1, 10, 32, 46]) [32,46]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "in_channels = 1\n",
    "out_channels = 10\n",
    "\n",
    "dilation = (1, 1)\n",
    "kernel_size = (3, 3)\n",
    "stride = (2, 2)\n",
    "padding = (1, 1)\n",
    "output_padding = (1, 1)\n",
    "h, w = 16, 23\n",
    "\n",
    "def cal_conv_out(h, padding, kernel_size, stride, dilation, output_padding):\n",
    "    # Transposed Convolution 연산을 통해 출력 크기 계산\n",
    "    size = (h - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1\n",
    "    return size \n",
    "\n",
    "# Transposed Convolution 레이어 생성\n",
    "conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, \n",
    "                          output_padding=output_padding, dilation=dilation)\n",
    "\n",
    "# 입력 데이터 생성: 크기가 (1, in_channels, h, w)인 텐서\n",
    "x = torch.ones((1, in_channels, h, w))\n",
    "\n",
    "# Transposed Convolution 레이어 통과한 출력 계산\n",
    "y = conv(x)\n",
    "\n",
    "# 계산된 출력의 높이와 너비\n",
    "output_h = cal_conv_out(h, padding[0], kernel_size[0], stride[0], dilation[0], output_padding[0])\n",
    "output_w = cal_conv_out(w, padding[1], kernel_size[1], stride[1], dilation[1], output_padding[1])\n",
    "\n",
    "# 입력과 출력의 형태, 출력의 높이와 너비 출력\n",
    "print(x.shape, y.shape, f'[{output_h}, {output_w}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-channel convolution\n",
    "\n",
    "<img src = 'img/torch_conv2d.png' width='800'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output size of 1D-Convolution Layer\n",
    "`Conv2d with Stride=2 and padding`=$\\frac{kernel}{2}$ means `downsampling` by $\\frac{1}{2}$.\n",
    "\n",
    "<img src=\"./img/conv1d_shape.png\" width=900>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 10]) torch.Size([1, 10, 5]) [5]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# 입력 채널, 출력 채널, dilation, 커널 크기, 스트라이드, 패딩 설정\n",
    "in_channels = 1\n",
    "out_channels = 10\n",
    "dilation = 1\n",
    "kernel_size = 3\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "# 입력 데이터 크기 설정\n",
    "input_size = 10\n",
    "\n",
    "# Convolution 연산 후 출력 크기 계산 함수\n",
    "def cal_conv_out(input_size, padding, kernel_size, stride, dilation):\n",
    "    size = int(np.floor((input_size + 2 * padding - dilation * (kernel_size - 1) - 1) / stride + 1))\n",
    "    return size \n",
    "\n",
    "# Conv1d 레이어 생성\n",
    "conv = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, \n",
    "                 dilation=dilation)\n",
    "\n",
    "# 입력 데이터 생성: 배치 크기 1, 입력 채널, 데이터 크기\n",
    "x = torch.ones((1, in_channels, input_size))\n",
    "\n",
    "# Conv1d 레이어를 통과한 출력 계산\n",
    "y = conv(x)\n",
    "\n",
    "# Convolution 연산을 통해 계산된 출력의 크기\n",
    "output_size = cal_conv_out(input_size, padding, kernel_size, stride, dilation)\n",
    "print(x.shape, y.shape, f'[{output_size}]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvTranspose1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 10]) torch.Size([1, 10, 20]) [20]\n"
     ]
    }
   ],
   "source": [
    "in_channels= 1  # 입력 데이터의 채널 수\n",
    "out_channels= 10  # 출력 데이터의 채널 수\n",
    "dilation = 1  # 실제 필터 값 사이의 간격을 결정하는 요소\n",
    "kernel_size = 3  # 컨볼루션 커널(필터)의 크기\n",
    "stride = 2  # 컨볼루션 연산 시 이동하는 보폭\n",
    "padding = 1  # 입력 데이터 주변을 둘러싸는 0 값의 개수\n",
    "output_padding = 1  # 출력 데이터 주변을 둘러싸는 0 값의 개수\n",
    "input_size = 10  # 입력 데이터의 크기\n",
    "\n",
    "# 컨볼루션 연산 후의 출력 크기를 계산하는 함수 정의\n",
    "def cal_conv_out(input_size, padding, kernel_size, stride, dilation, output_padding):\n",
    "        size = (input_size-1)*stride - 2*padding + dilation*(kernel_size-1) + output_padding + 1\n",
    "        return size \n",
    "\n",
    "# nn.ConvTranspose1d 모듈을 사용하여 컨볼루션 레이어 정의\n",
    "conv = nn.ConvTranspose1d(in_channels, out_channels, kernel_size= kernel_size, stride=stride, padding= padding, \n",
    "                          output_padding=output_padding,  dilation=dilation)\n",
    "\n",
    "x = torch.ones((1, in_channels, input_size))  # 입력 데이터 생성\n",
    "y = conv(x)  # 입력 데이터를 컨볼루션 레이어에 통과시켜 출력 데이터 생성\n",
    "\n",
    "# 실제 출력 크기 계산\n",
    "output_size = cal_conv_out(input_size, padding, kernel_size, stride, dilation, output_padding)\n",
    "\n",
    "# 입력, 출력 데이터의 형태 및 실제 출력 크기 출력\n",
    "print(x.shape, y.shape, f'[{output_size}]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9970af6c6272f912aea970d3aa30162bd36dbb3ebdad53a40420a930d21aa1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
