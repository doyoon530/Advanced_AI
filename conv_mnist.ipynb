{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "#1.모듈로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "learning_rate=1e-4\n",
    "epochs=100\n",
    " #2.하이퍼 파라미터\n",
    "\n",
    "dir = Path.joinpath(list(Path.cwd().parents)[2], \"00. DeepLearning/00_Dataset\")\n",
    "ds_train = torchvision.datasets.MNIST(root=dir, \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "ds_test = torchvision.datasets.MNIST(root=dir, \n",
    "                                     train=False, \n",
    "                                     transform=transforms.ToTensor(),\n",
    "                                     download=True)\n",
    "\n",
    "dl_train=torch.utils.data.DataLoader(ds_train,batch_size,shuffle=True,drop_last=False)\n",
    "dl_test=torch.utils.data.DataLoader(ds_test,batch_size,shuffle=True,drop_last=False)\n",
    "#3.dataset정의\n",
    "\n",
    "fig, axes = plt.subplots(1,10,figsize=(20, 2))\n",
    "for i in range(10):\n",
    "    img = ds_train[i][0]\n",
    "    label = ds_train[i][1]\n",
    "    axes[i].imshow(img.squeeze(), cmap='gray')\n",
    "\n",
    "conv = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten()\n",
    ")\n",
    "\n",
    "input = torch.ones((1,1,28,28), dtype=torch.float32)\n",
    "out = conv(input)\n",
    "conv_out_size = torch.numel(out)\n",
    "print(conv_out_size)\n",
    "\n",
    "lin = nn.Sequential(\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),  # 드롭아웃 추가\n",
    "    nn.Linear(in_features=128, out_features=10)\n",
    ")\n",
    "\n",
    "model = nn.Sequential(conv,lin)\n",
    "out = model(input)\n",
    "\n",
    "#4.NN정의\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    model.train()\n",
    "    return accuracy\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "#6.opt & loss_criterion\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    loss_avg = 0\n",
    "    for i, (imgs, targets) in enumerate(dl_train):\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        out = model(imgs)\n",
    "        loss = loss_criterion(out, targets)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_avg += loss.item()\n",
    "    loss_avg = loss_avg / (i + 1)\n",
    "    losses.append(loss_avg)\n",
    "    if (epoch+1)%10==0:\n",
    "        acc=evaluate(dl_test)\n",
    "        print(f\"Epoch {epoch + 1}; acc: {acc:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}; Loss: {loss_avg:.4f}\")\n",
    "#7.train & 8.evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = Path.joinpath(list(Path.cwd().parents)[2], \"00. DeepLearning/00_Dataset\")\n",
    "ds_train = torchvision.datasets.MNIST(root=dir, \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "ds_test = torchvision.datasets.MNIST(root=dir, \n",
    "                                     train=False, \n",
    "                                     transform=transforms.ToTensor(),\n",
    "                                     download=True)\n",
    "\n",
    "dl_train=torch.utils.data.DataLoader(ds_train,batch_size,shuffle=True,drop_last=False)\n",
    "dl_test=torch.utils.data.DataLoader(ds_test,batch_size,shuffle=True,drop_last=False)\n",
    "#3.dataset정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1,10,figsize=(20, 2))\n",
    "for i in range(10):\n",
    "    img = ds_train[i][0]\n",
    "    label = ds_train[i][1]\n",
    "    axes[i].imshow(img.squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "conv = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=128, kernel_size=3, stride=2, padding=1), #14x14\n",
    "    nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1), #7x7\n",
    "    nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1),  #4x4\n",
    "    nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1),  #2x2\n",
    "    nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1),  #1x1\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=128, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=10),\n",
    ")\n",
    "\n",
    "input = torch.ones((1,1,28,28), dtype=torch.float32)\n",
    "out = conv(input)\n",
    "conv_out_size = torch.numel(out)\n",
    "print(conv_out_size)\n",
    "\n",
    "lin = nn.Sequential(\n",
    "    nn.Linear(in_features=conv_out_size, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=10),\n",
    ")\n",
    "\n",
    "model = nn.Sequential(conv,lin)\n",
    "out = model(input)\n",
    "\n",
    "#4.NN정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    model.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1; Loss: 0.7913\n",
      "Epoch 2; Loss: 0.3141\n",
      "Epoch 3; Loss: 0.2430\n",
      "Epoch 4; Loss: 0.2034\n",
      "Epoch 5; Loss: 0.1748\n",
      "Epoch 6; Loss: 0.1539\n",
      "Epoch 7; Loss: 0.1392\n",
      "Epoch 8; Loss: 0.1255\n",
      "Epoch 9; Loss: 0.1146\n",
      "Epoch 10; acc: 0.9655\n",
      "Epoch 10; Loss: 0.1071\n",
      "Epoch 11; Loss: 0.1002\n",
      "Epoch 12; Loss: 0.0928\n",
      "Epoch 13; Loss: 0.0874\n",
      "Epoch 14; Loss: 0.0824\n",
      "Epoch 15; Loss: 0.0793\n",
      "Epoch 16; Loss: 0.0733\n",
      "Epoch 17; Loss: 0.0715\n",
      "Epoch 18; Loss: 0.0682\n",
      "Epoch 19; Loss: 0.0650\n",
      "Epoch 20; acc: 0.9742\n",
      "Epoch 20; Loss: 0.0622\n",
      "Epoch 21; Loss: 0.0589\n",
      "Epoch 22; Loss: 0.0576\n",
      "Epoch 23; Loss: 0.0548\n",
      "Epoch 24; Loss: 0.0533\n",
      "Epoch 25; Loss: 0.0508\n",
      "Epoch 26; Loss: 0.0487\n",
      "Epoch 27; Loss: 0.0464\n",
      "Epoch 28; Loss: 0.0445\n",
      "Epoch 29; Loss: 0.0441\n",
      "Epoch 30; acc: 0.9762\n",
      "Epoch 30; Loss: 0.0420\n",
      "Epoch 31; Loss: 0.0390\n",
      "Epoch 32; Loss: 0.0399\n",
      "Epoch 33; Loss: 0.0371\n",
      "Epoch 34; Loss: 0.0367\n",
      "Epoch 35; Loss: 0.0345\n",
      "Epoch 36; Loss: 0.0334\n",
      "Epoch 37; Loss: 0.0327\n",
      "Epoch 38; Loss: 0.0313\n",
      "Epoch 39; Loss: 0.0300\n",
      "Epoch 40; acc: 0.9775\n",
      "Epoch 40; Loss: 0.0305\n",
      "Epoch 41; Loss: 0.0281\n",
      "Epoch 42; Loss: 0.0271\n",
      "Epoch 43; Loss: 0.0272\n",
      "Epoch 44; Loss: 0.0256\n",
      "Epoch 45; Loss: 0.0248\n",
      "Epoch 46; Loss: 0.0241\n",
      "Epoch 47; Loss: 0.0236\n",
      "Epoch 48; Loss: 0.0223\n",
      "Epoch 49; Loss: 0.0224\n",
      "Epoch 50; acc: 0.9769\n",
      "Epoch 50; Loss: 0.0209\n",
      "Epoch 51; Loss: 0.0203\n",
      "Epoch 52; Loss: 0.0206\n",
      "Epoch 53; Loss: 0.0191\n",
      "Epoch 54; Loss: 0.0217\n",
      "Epoch 55; Loss: 0.0181\n",
      "Epoch 56; Loss: 0.0156\n",
      "Epoch 57; Loss: 0.0177\n",
      "Epoch 58; Loss: 0.0190\n",
      "Epoch 59; Loss: 0.0164\n",
      "Epoch 60; acc: 0.9757\n",
      "Epoch 60; Loss: 0.0177\n",
      "Epoch 61; Loss: 0.0154\n",
      "Epoch 62; Loss: 0.0153\n",
      "Epoch 63; Loss: 0.0163\n",
      "Epoch 64; Loss: 0.0158\n",
      "Epoch 65; Loss: 0.0145\n",
      "Epoch 66; Loss: 0.0126\n",
      "Epoch 67; Loss: 0.0155\n",
      "Epoch 68; Loss: 0.0148\n",
      "Epoch 69; Loss: 0.0132\n",
      "Epoch 70; acc: 0.9772\n",
      "Epoch 70; Loss: 0.0128\n",
      "Epoch 71; Loss: 0.0130\n",
      "Epoch 72; Loss: 0.0136\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ggam/Desktop/gnu/Advanced_AI/conv_mnist.ipynb 셀 7\u001b[0m line \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ggam/Desktop/gnu/Advanced_AI/conv_mnist.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (imgs, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dl_train):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ggam/Desktop/gnu/Advanced_AI/conv_mnist.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     imgs, targets \u001b[39m=\u001b[39m imgs\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ggam/Desktop/gnu/Advanced_AI/conv_mnist.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     out \u001b[39m=\u001b[39m model(imgs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ggam/Desktop/gnu/Advanced_AI/conv_mnist.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_criterion(out, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ggam/Desktop/gnu/Advanced_AI/conv_mnist.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     opt\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "#6.opt & loss_criterion\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    loss_avg = 0\n",
    "    for i, (imgs, targets) in enumerate(dl_train):\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        out = model(imgs)\n",
    "        loss = loss_criterion(out, targets)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_avg += loss.item()\n",
    "    loss_avg = loss_avg / (i + 1)\n",
    "    losses.append(loss_avg)\n",
    "    if (epoch+1)%10==0:\n",
    "        acc=evaluate(dl_test)\n",
    "        print(f\"Epoch {epoch + 1}; acc: {acc:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}; Loss: {loss_avg:.4f}\")\n",
    "#7.train & 8.evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
